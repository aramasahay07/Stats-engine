{
  "version": "v2-full-fields",
  "generated_at": "2025-12-20T03:18:17.941239+00:00",
  "topics": [
    {
      "slug": "foundations",
      "title": "Foundations",
      "concepts": [
        {
          "slug": "what-is-data",
          "title": "What is Data",
          "definition": "Recorded observations or measurements collected for analysis and decision-making.",
          "plain_english": "Information you collect to learn what's happening.",
          "when_to_use": "Use when defining the problem and what you need to measure.",
          "tags": [
            "data",
            "measurement"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "population-vs-sample",
          "title": "Population vs Sample",
          "definition": "A population is the full set of units of interest; a sample is a subset used to learn about the population.",
          "plain_english": "You usually measure a part, then infer about the whole.",
          "when_to_use": "Use whenever you generalize from measured data to a broader group.",
          "tags": [
            "sampling"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "types-of-variables",
          "title": "Types of Variables",
          "definition": "Variables are attributes that vary; common types are categorical (labels) and numerical (quantities).",
          "plain_english": "Some columns are categories, others are numbers.",
          "when_to_use": "Use when choosing summaries, charts, and statistical tests.",
          "tags": [
            "variables"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "levels-of-measurement",
          "title": "Levels of Measurement",
          "definition": "Measurement levels (nominal, ordinal, interval, ratio) describe what operations and comparisons are valid for a variable.",
          "plain_english": "It tells you what math makes sense for a column.",
          "when_to_use": "Use to avoid invalid averages or differences (e.g., averaging IDs).",
          "tags": [
            "measurement"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "discrete-vs-continuous",
          "title": "Discrete vs Continuous Data",
          "definition": "Discrete data takes countable values; continuous data can take any value within a range.",
          "plain_english": "Counts vs measurements on a scale.",
          "when_to_use": "Use when selecting distributions and modeling choices.",
          "tags": [
            "data-types"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "structured-vs-unstructured",
          "title": "Structured vs Unstructured Data",
          "definition": "Structured data fits rows/columns; unstructured data includes text, images, audio, and free-form content.",
          "plain_english": "Tables vs messy content like notes or documents.",
          "when_to_use": "Use when planning storage, preprocessing, and analysis tooling.",
          "tags": [
            "data-types"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "observational-vs-experimental",
          "title": "Observational vs Experimental Data",
          "definition": "Observational data records what happened naturally; experimental data results from deliberate interventions with control.",
          "plain_english": "Watching reality vs running a controlled change.",
          "when_to_use": "Use when deciding how strongly you can claim cause-and-effect.",
          "tags": [
            "causality"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        }
      ]
    },
    {
      "slug": "descriptive-statistics",
      "title": "Descriptive Statistics",
      "concepts": [
        {
          "slug": "mean",
          "title": "Mean",
          "definition": "The arithmetic average: sum of values divided by the number of values.",
          "plain_english": "The typical value if everything were evenly spread.",
          "when_to_use": "Use to summarize central tendency for roughly symmetric numerical data.",
          "tags": [
            "center"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "mean",
            "avg"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "median",
          "title": "Median",
          "definition": "The middle value when data are ordered (50th percentile).",
          "plain_english": "The 'middle' result, less sensitive to outliers than the mean.",
          "when_to_use": "Use when data are skewed or have outliers.",
          "tags": [
            "center",
            "robust"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "median"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "mode",
          "title": "Mode",
          "definition": "The most frequent value in a dataset.",
          "plain_english": "The value that shows up the most.",
          "when_to_use": "Use for categorical data or to find common repeated values.",
          "tags": [
            "center"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "mode"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "range",
          "title": "Range",
          "definition": "Maximum minus minimum value.",
          "plain_english": "How wide the values spread from low to high.",
          "when_to_use": "Use as a quick sense of spread (often alongside more robust measures).",
          "tags": [
            "spread"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "range"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "variance",
          "title": "Variance",
          "definition": "Average squared deviation from the mean; the square of standard deviation.",
          "plain_english": "Spread measured in squared units; SD is usually easier to interpret.",
          "when_to_use": "Use in modeling and as a building block for SD and inference.",
          "tags": [
            "spread"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "variance",
            "var"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "reduce",
            "typical_target_reduction_pct": 20,
            "levers": [
              "segment data to find high-variation groups",
              "remove special-cause outliers",
              "standardize inputs/procedures",
              "tighten controls/checklists"
            ],
            "warnings": [
              "Do not reduce variation by lowering quality or shifting burden downstream",
              "Confirm variation is not mostly case-mix driven"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "standard-deviation",
          "title": "Standard Deviation",
          "definition": "Square root of variance; typical distance of values from the mean.",
          "plain_english": "How consistent results are around the average.",
          "when_to_use": "Use to quantify variability and compare stability across groups/time.",
          "tags": [
            "spread",
            "variation"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "std_dev",
            "standard_deviation",
            "sd"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "reduce",
            "typical_target_reduction_pct": 20,
            "levers": [
              "segment data to find high-variation groups",
              "remove special-cause outliers",
              "standardize inputs/procedures",
              "tighten controls/checklists"
            ],
            "warnings": [
              "Do not reduce variation by lowering quality or shifting burden downstream",
              "Confirm variation is not mostly case-mix driven"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "interquartile-range",
          "title": "Interquartile Range (IQR)",
          "definition": "Difference between the 75th and 25th percentiles (Q3 − Q1).",
          "plain_english": "Spread of the middle half of your data.",
          "when_to_use": "Use to describe typical variability and detect outliers robustly.",
          "tags": [
            "spread",
            "robust"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "iqr"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "reduce",
            "typical_target_reduction_pct": 20,
            "levers": [
              "segment data to find high-variation groups",
              "remove special-cause outliers",
              "standardize inputs/procedures",
              "tighten controls/checklists"
            ],
            "warnings": [
              "Do not reduce variation by lowering quality or shifting burden downstream",
              "Confirm variation is not mostly case-mix driven"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "percentiles",
          "title": "Percentiles",
          "definition": "Values below which a given percentage of observations fall (e.g., 90th percentile).",
          "plain_english": "A way to talk about the 'top' or 'bottom' of performance.",
          "when_to_use": "Use for service levels (p90 wait time), tail risk, and benchmarking.",
          "tags": [
            "distribution"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "percentile",
            "pctl"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Data-generating process is reasonably stable; extreme outliers/special causes can distort fit."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "quantiles",
          "title": "Quantiles",
          "definition": "Cut points that divide the distribution into equal-sized intervals (quartiles, deciles).",
          "plain_english": "Percentiles grouped into evenly sized buckets.",
          "when_to_use": "Use to summarize distributions and compare groups.",
          "tags": [
            "distribution"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "quantile"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Data-generating process is reasonably stable; extreme outliers/special causes can distort fit."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "z-score",
          "title": "Z-score",
          "definition": "Number of standard deviations a value is from the mean: (x−μ)/σ.",
          "plain_english": "How unusual a value is compared to typical behavior.",
          "when_to_use": "Use for outlier detection and standardization across scales.",
          "tags": [
            "standardization"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "z_score",
            "zscore"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "coefficient-of-variation",
          "title": "Coefficient of Variation (CV)",
          "definition": "Standard deviation divided by mean; a scale-free measure of relative variability.",
          "plain_english": "Variation compared to the average (good for comparing different units).",
          "when_to_use": "Use to compare variability across metrics with different scales.",
          "tags": [
            "spread"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "cv",
            "coef_var"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "reduce",
            "typical_target_reduction_pct": 20,
            "levers": [
              "segment data to find high-variation groups",
              "remove special-cause outliers",
              "standardize inputs/procedures",
              "tighten controls/checklists"
            ],
            "warnings": [
              "Do not reduce variation by lowering quality or shifting burden downstream",
              "Confirm variation is not mostly case-mix driven"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "mad",
          "title": "Median Absolute Deviation (MAD)",
          "definition": "Median of absolute deviations from the median; robust variability measure.",
          "plain_english": "A robust version of SD that ignores extreme outliers.",
          "when_to_use": "Use when outliers or heavy tails make SD misleading.",
          "tags": [
            "robust",
            "spread"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "mad"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "reduce",
            "typical_target_reduction_pct": 20,
            "levers": [
              "segment data to find high-variation groups",
              "remove special-cause outliers",
              "standardize inputs/procedures",
              "tighten controls/checklists"
            ],
            "warnings": [
              "Do not reduce variation by lowering quality or shifting burden downstream",
              "Confirm variation is not mostly case-mix driven"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "skewness",
          "title": "Skewness",
          "definition": "Measure of asymmetry of a distribution.",
          "plain_english": "Whether data have a long tail to the right or left.",
          "when_to_use": "Use to choose appropriate summaries and transformations.",
          "tags": [
            "shape"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "skewness",
            "skew"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "kurtosis",
          "title": "Kurtosis",
          "definition": "Measure related to tail heaviness and peak relative to normal distribution.",
          "plain_english": "How extreme the extremes are compared to normal.",
          "when_to_use": "Use to assess tail risk and suitability of normal assumptions.",
          "tags": [
            "shape"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "kurtosis"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "outliers",
          "title": "Outliers",
          "definition": "Observations that are unusually far from the rest of the data (e.g., beyond 1.5×IQR or high |z|).",
          "plain_english": "Extreme values that may signal special causes or data issues.",
          "when_to_use": "Use to detect data errors, special causes, and tail risk; handle with care.",
          "tags": [
            "quality",
            "variation"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "outliers",
            "outlier"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        }
      ]
    },
    {
      "slug": "visualization-eda",
      "title": "Visualization & EDA",
      "concepts": [
        {
          "slug": "histogram",
          "title": "Histogram",
          "definition": "Bar-like plot showing counts (or density) across value bins.",
          "plain_english": "Shows the shape of your data.",
          "when_to_use": "Use to see distribution shape, skew, and outliers.",
          "tags": [
            "visual"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "chart",
          "output_keys": [
            "histogram"
          ],
          "improvement_goal": "Use this visualization to spot patterns and variation.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "choose the right chart for the question",
              "label axes/units",
              "show distribution (not only averages)",
              "avoid misleading scales"
            ],
            "warnings": [
              "Bad visuals create bad decisions"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "box-plot",
          "title": "Box Plot",
          "definition": "Plot summarizing median, quartiles, and potential outliers.",
          "plain_english": "A quick view of typical range and extremes.",
          "when_to_use": "Use to compare distributions across groups.",
          "tags": [
            "visual"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "chart",
          "output_keys": [
            "boxplot",
            "box_plot"
          ],
          "improvement_goal": "Use this visualization to spot patterns and variation.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "choose the right chart for the question",
              "label axes/units",
              "show distribution (not only averages)",
              "avoid misleading scales"
            ],
            "warnings": [
              "Bad visuals create bad decisions"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "bar-chart",
          "title": "Bar Chart",
          "definition": "Chart showing counts or aggregated values for categories.",
          "plain_english": "Compare categories side-by-side.",
          "when_to_use": "Use for categorical comparisons (counts, rates, means).",
          "tags": [
            "visual"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "chart",
          "output_keys": [
            "bar_chart"
          ],
          "improvement_goal": "Use this visualization to spot patterns and variation.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "choose the right chart for the question",
              "label axes/units",
              "show distribution (not only averages)",
              "avoid misleading scales"
            ],
            "warnings": [
              "Bad visuals create bad decisions"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "line-chart",
          "title": "Line Chart",
          "definition": "Plot of values over an ordered dimension (usually time).",
          "plain_english": "Shows trends over time.",
          "when_to_use": "Use for time-based patterns, seasonality, and changes.",
          "tags": [
            "visual",
            "time"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "chart",
          "output_keys": [
            "line_chart"
          ],
          "improvement_goal": "Use this visualization to spot patterns and variation.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "choose the right chart for the question",
              "label axes/units",
              "show distribution (not only averages)",
              "avoid misleading scales"
            ],
            "warnings": [
              "Bad visuals create bad decisions"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "scatter-plot",
          "title": "Scatter Plot",
          "definition": "Plot of two numerical variables to inspect relationship.",
          "plain_english": "Shows whether two things move together.",
          "when_to_use": "Use to assess association, nonlinearity, and outliers.",
          "tags": [
            "visual",
            "relationship"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "chart",
          "output_keys": [
            "scatter_plot"
          ],
          "improvement_goal": "Use this visualization to spot patterns and variation.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "choose the right chart for the question",
              "label axes/units",
              "show distribution (not only averages)",
              "avoid misleading scales"
            ],
            "warnings": [
              "Bad visuals create bad decisions"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "density-plot",
          "title": "Density Plot",
          "definition": "Smoothed estimate of the distribution of a variable.",
          "plain_english": "A smoother histogram.",
          "when_to_use": "Use to compare distribution shapes and multi-modality.",
          "tags": [
            "visual"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "chart",
          "output_keys": [
            "density_plot",
            "kde"
          ],
          "improvement_goal": "Use this visualization to spot patterns and variation.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "choose the right chart for the question",
              "label axes/units",
              "show distribution (not only averages)",
              "avoid misleading scales"
            ],
            "warnings": [
              "Bad visuals create bad decisions"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "pair-plot",
          "title": "Pair Plot",
          "definition": "Grid of scatterplots/histograms for multiple variables.",
          "plain_english": "Quickly scans many relationships at once.",
          "when_to_use": "Use during EDA to spot correlations and clusters.",
          "tags": [
            "visual",
            "eda"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "chart",
          "output_keys": [
            "pair_plot"
          ],
          "improvement_goal": "Use this visualization to spot patterns and variation.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "choose the right chart for the question",
              "label axes/units",
              "show distribution (not only averages)",
              "avoid misleading scales"
            ],
            "warnings": [
              "Bad visuals create bad decisions"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "exploratory-data-analysis",
          "title": "Exploratory Data Analysis (EDA)",
          "definition": "Process of profiling data quality, distributions, relationships, and anomalies before modeling.",
          "plain_english": "The first pass to understand what’s in your data.",
          "when_to_use": "Use at the start of every analysis to avoid garbage-in/garbage-out.",
          "tags": [
            "eda",
            "quality"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "eda"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        }
      ]
    },
    {
      "slug": "probability",
      "title": "Probability",
      "concepts": [
        {
          "slug": "probability-basics",
          "title": "Probability Basics",
          "definition": "A number from 0 to 1 representing likelihood of an event.",
          "plain_english": "How likely something is to happen.",
          "when_to_use": "Use to quantify uncertainty and risk.",
          "tags": [
            "probability"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [
            "probability"
          ],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "sample-space",
          "title": "Sample Space",
          "definition": "Set of all possible outcomes of a random process.",
          "plain_english": "All the outcomes that could happen.",
          "when_to_use": "Use when defining events and calculating probabilities.",
          "tags": [
            "probability"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "events",
          "title": "Events",
          "definition": "A subset of the sample space representing outcomes of interest.",
          "plain_english": "The outcome(s) you care about.",
          "when_to_use": "Use to formalize probability questions.",
          "tags": [
            "probability"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "independent-vs-dependent-events",
          "title": "Independent vs Dependent Events",
          "definition": "Independent events do not affect each other’s probabilities; dependent events do.",
          "plain_english": "Whether one thing changes the chance of another.",
          "when_to_use": "Use for risk calculations and conditional logic.",
          "tags": [
            "probability"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [
            "independence"
          ],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "conditional-probability",
          "title": "Conditional Probability",
          "definition": "Probability of event A given event B occurred: P(A|B).",
          "plain_english": "Chance of A when you already know B is true.",
          "when_to_use": "Use for diagnostic reasoning and segmentation.",
          "tags": [
            "probability"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [
            "conditional_probability"
          ],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "bayes-theorem",
          "title": "Bayes’ Theorem",
          "definition": "Rule that updates beliefs using evidence: P(A|B)=P(B|A)P(A)/P(B).",
          "plain_english": "How you update what you believe after new evidence.",
          "when_to_use": "Use in diagnostic testing, prediction updates, and risk scoring.",
          "tags": [
            "probability",
            "bayesian"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "bayes"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "uniform-distribution",
          "title": "Uniform Distribution",
          "definition": "Distribution where all outcomes in a range are equally likely.",
          "plain_english": "Everything in the range is equally likely.",
          "when_to_use": "Use as a simple baseline distribution.",
          "tags": [
            "distribution"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "distribution",
          "output_keys": [
            "uniform"
          ],
          "improvement_goal": "Use this concept to improve decisions and reduce uncertainty.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Data-generating process is reasonably stable; extreme outliers/special causes can distort fit."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "binomial-distribution",
          "title": "Binomial Distribution",
          "definition": "Distribution of number of successes in n independent Bernoulli trials with probability p.",
          "plain_english": "Counts how many successes out of n tries.",
          "when_to_use": "Use for yes/no outcomes across repeated trials.",
          "tags": [
            "distribution"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "distribution",
          "output_keys": [
            "binomial"
          ],
          "improvement_goal": "Use this concept to improve decisions and reduce uncertainty.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Data-generating process is reasonably stable; extreme outliers/special causes can distort fit."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "poisson-distribution",
          "title": "Poisson Distribution",
          "definition": "Distribution of counts of events in a fixed interval with a constant average rate.",
          "plain_english": "How many events happen in a time/window.",
          "when_to_use": "Use for arrivals, defects, or rare events counts.",
          "tags": [
            "distribution"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "distribution",
          "output_keys": [
            "poisson"
          ],
          "improvement_goal": "Use this concept to improve decisions and reduce uncertainty.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Data-generating process is reasonably stable; extreme outliers/special causes can distort fit."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "normal-distribution",
          "title": "Normal Distribution",
          "definition": "Bell-shaped continuous distribution defined by mean and standard deviation.",
          "plain_english": "The classic bell curve.",
          "when_to_use": "Use when modeling many natural measurement processes or as an approximation via CLT.",
          "tags": [
            "distribution"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "distribution",
          "output_keys": [
            "normal",
            "gaussian"
          ],
          "improvement_goal": "Use this concept to improve decisions and reduce uncertainty.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Data-generating process is reasonably stable; extreme outliers/special causes can distort fit."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "exponential-distribution",
          "title": "Exponential Distribution",
          "definition": "Continuous distribution of time between Poisson events; memoryless.",
          "plain_english": "Time between events when arrivals are random.",
          "when_to_use": "Use for inter-arrival times and waiting-time modeling.",
          "tags": [
            "distribution",
            "time"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "distribution",
          "output_keys": [
            "exponential"
          ],
          "improvement_goal": "Use this concept to improve decisions and reduce uncertainty.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Data-generating process is reasonably stable; extreme outliers/special causes can distort fit."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        }
      ]
    },
    {
      "slug": "sampling-estimation",
      "title": "Sampling & Estimation",
      "concepts": [
        {
          "slug": "sampling-methods",
          "title": "Sampling Methods",
          "definition": "Techniques to select a sample (random, stratified, cluster) to represent a population.",
          "plain_english": "How you choose what to measure.",
          "when_to_use": "Use to improve representativeness and reduce bias.",
          "tags": [
            "sampling"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "sampling_methods"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "sampling-bias",
          "title": "Sampling Bias",
          "definition": "Systematic difference between the sample and the population that distorts estimates.",
          "plain_english": "If you sample the wrong way, you learn the wrong thing.",
          "when_to_use": "Use to assess whether results generalize.",
          "tags": [
            "sampling",
            "bias"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [
            "sampling_bias"
          ],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "sampling-distribution",
          "title": "Sampling Distribution",
          "definition": "Distribution of a statistic (like the mean) over repeated samples from the population.",
          "plain_english": "How an estimate would vary if you repeated the sampling.",
          "when_to_use": "Use to understand uncertainty in estimates.",
          "tags": [
            "inference"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [
            "sampling_distribution"
          ],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "central-limit-theorem",
          "title": "Central Limit Theorem (CLT)",
          "definition": "For large samples, the sampling distribution of the mean approaches normal, regardless of population shape (with conditions).",
          "plain_english": "Averages become bell-shaped when you sample enough.",
          "when_to_use": "Use to justify normal-based confidence intervals/tests.",
          "tags": [
            "inference"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [
            "clt"
          ],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "point-estimates",
          "title": "Point Estimates",
          "definition": "Single-number estimate of a population parameter (e.g., sample mean for population mean).",
          "plain_english": "Your best single guess of the true value.",
          "when_to_use": "Use when reporting a parameter estimate before adding uncertainty.",
          "tags": [
            "inference"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [
            "point_estimate"
          ],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "confidence-intervals",
          "title": "Confidence Intervals",
          "definition": "Range of plausible values for a population parameter with a stated confidence level.",
          "plain_english": "A likely range for the true value.",
          "when_to_use": "Use to quantify uncertainty around estimates.",
          "tags": [
            "inference"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "confidence_interval",
            "ci"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "ci-mean",
          "title": "CI for Mean",
          "definition": "Confidence interval estimating a population mean, typically using t-distribution when σ is unknown.",
          "plain_english": "Range where the true average likely falls.",
          "when_to_use": "Use when estimating average outcomes with uncertainty.",
          "tags": [
            "inference"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "ci_mean"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "ci-proportion",
          "title": "CI for Proportion",
          "definition": "Confidence interval estimating a population proportion (rate).",
          "plain_english": "Range for the true rate (e.g., error rate).",
          "when_to_use": "Use for rates like defect %, readmission %, compliance %.",
          "tags": [
            "inference"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "ci_proportion"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "sample-size",
          "title": "Sample Size Planning",
          "definition": "Process of selecting n to achieve desired precision or power given variability and effect size.",
          "plain_english": "How many data points you need to trust the result.",
          "when_to_use": "Use before experiments or studies to avoid underpowered decisions.",
          "tags": [
            "planning",
            "power"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "sample_size",
            "n_required"
          ],
          "improvement_goal": "Make better decisions by separating real effects from random noise.",
          "diagnostic_questions": [
            "Is the effect size meaningful operationally?",
            "Are assumptions and data quality acceptable?",
            "How many hypotheses are being tested?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        }
      ]
    },
    {
      "slug": "hypothesis-testing",
      "title": "Hypothesis Testing",
      "concepts": [
        {
          "slug": "null-vs-alternative",
          "title": "Null vs Alternative Hypothesis",
          "definition": "Null hypothesis represents no effect; alternative represents a meaningful effect or difference.",
          "plain_english": "The default assumption vs what you're trying to prove.",
          "when_to_use": "Use to frame statistical tests clearly.",
          "tags": [
            "testing"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [
            "null_hypothesis",
            "alternative_hypothesis"
          ],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "type-i-error",
          "title": "Type I Error",
          "definition": "Rejecting a true null hypothesis (false positive).",
          "plain_english": "Calling a change real when it isn’t.",
          "when_to_use": "Use to weigh the cost of false alarms.",
          "tags": [
            "testing",
            "risk"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [
            "type_i_error",
            "false_positive"
          ],
          "improvement_goal": "Make better decisions by separating real effects from random noise.",
          "diagnostic_questions": [
            "Is the effect size meaningful operationally?",
            "Are assumptions and data quality acceptable?",
            "How many hypotheses are being tested?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "type-ii-error",
          "title": "Type II Error",
          "definition": "Failing to reject a false null hypothesis (false negative).",
          "plain_english": "Missing a real change because you lacked evidence.",
          "when_to_use": "Use to weigh the cost of missed opportunities.",
          "tags": [
            "testing",
            "risk"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [
            "type_ii_error",
            "false_negative"
          ],
          "improvement_goal": "Make better decisions by separating real effects from random noise.",
          "diagnostic_questions": [
            "Is the effect size meaningful operationally?",
            "Are assumptions and data quality acceptable?",
            "How many hypotheses are being tested?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "significance-level",
          "title": "Significance Level (α)",
          "definition": "Pre-chosen threshold for Type I error rate used to decide statistical significance.",
          "plain_english": "How strict you are about avoiding false alarms.",
          "when_to_use": "Use to set decision rules before running a test.",
          "tags": [
            "testing"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "alpha"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "p-value",
          "title": "p-value",
          "definition": "Probability of observing results at least as extreme as the data, assuming the null hypothesis is true.",
          "plain_english": "How surprising your result would be if nothing changed.",
          "when_to_use": "Use to quantify evidence against the null (with effect size and context).",
          "tags": [
            "testing"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "p_value",
            "pvalue"
          ],
          "improvement_goal": "Make better decisions by separating real effects from random noise.",
          "diagnostic_questions": [
            "Is the effect size meaningful operationally?",
            "Are assumptions and data quality acceptable?",
            "How many hypotheses are being tested?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "p-values do not measure the size of an effect or the probability the null is true; interpret with caution."
          ]
        },
        {
          "slug": "power",
          "title": "Statistical Power",
          "definition": "Probability of correctly rejecting the null when the alternative is true (1−β).",
          "plain_english": "How likely you are to detect a real effect.",
          "when_to_use": "Use for study planning and interpreting null results.",
          "tags": [
            "testing"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "power"
          ],
          "improvement_goal": "Make better decisions by separating real effects from random noise.",
          "diagnostic_questions": [
            "Is the effect size meaningful operationally?",
            "Are assumptions and data quality acceptable?",
            "How many hypotheses are being tested?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "effect-size",
          "title": "Effect Size",
          "definition": "Magnitude of a difference or relationship, independent of sample size.",
          "plain_english": "How big the impact really is.",
          "when_to_use": "Use alongside p-values to judge practical importance.",
          "tags": [
            "testing",
            "impact"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "effect_size",
            "cohens_d",
            "risk_ratio"
          ],
          "improvement_goal": "Make better decisions by separating real effects from random noise.",
          "diagnostic_questions": [
            "Is the effect size meaningful operationally?",
            "Are assumptions and data quality acceptable?",
            "How many hypotheses are being tested?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "multiple-testing",
          "title": "Multiple Testing",
          "definition": "Testing many hypotheses increases the chance of false positives unless adjusted.",
          "plain_english": "If you test lots of things, some look 'significant' by luck.",
          "when_to_use": "Use when exploring many variables or segments.",
          "tags": [
            "testing"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [
            "multiple_testing"
          ],
          "improvement_goal": "Make better decisions by separating real effects from random noise.",
          "diagnostic_questions": [
            "Is the effect size meaningful operationally?",
            "Are assumptions and data quality acceptable?",
            "How many hypotheses are being tested?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "bonferroni",
          "title": "Bonferroni Correction",
          "definition": "Adjustment that controls family-wise error by using α/m for m tests.",
          "plain_english": "A stricter threshold when you run many tests.",
          "when_to_use": "Use when false positives are very costly.",
          "tags": [
            "testing"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "bonferroni"
          ],
          "improvement_goal": "Make better decisions by separating real effects from random noise.",
          "diagnostic_questions": [
            "Is the effect size meaningful operationally?",
            "Are assumptions and data quality acceptable?",
            "How many hypotheses are being tested?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "fdr",
          "title": "False Discovery Rate (FDR)",
          "definition": "Expected proportion of false positives among rejected hypotheses.",
          "plain_english": "How many 'discoveries' might be wrong.",
          "when_to_use": "Use in high-dimensional testing where you want balance between discovery and caution.",
          "tags": [
            "testing"
          ],
          "level": "advanced",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "fdr",
            "benjamini_hochberg"
          ],
          "improvement_goal": "Make better decisions by separating real effects from random noise.",
          "diagnostic_questions": [
            "Is the effect size meaningful operationally?",
            "Are assumptions and data quality acceptable?",
            "How many hypotheses are being tested?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "z-test",
          "title": "Z-test",
          "definition": "Test comparing a mean/proportion to a known value using normal distribution assumptions (often large n).",
          "plain_english": "A test when sample size is big and variability is known/approximated.",
          "when_to_use": "Use for proportions and large-sample mean testing.",
          "tags": [
            "test"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "test",
          "output_keys": [
            "z_test"
          ],
          "improvement_goal": "Use this test to validate whether a change is real.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "validate",
            "levers": [
              "check assumptions",
              "report effect size + confidence interval",
              "predefine hypotheses",
              "confirm with new data"
            ],
            "warnings": [
              "Statistical significance alone is not practical significance",
              "Multiple testing inflates false positives"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "one-sample-t-test",
          "title": "One-sample t-test",
          "definition": "Test whether a sample mean differs from a hypothesized value using t-distribution.",
          "plain_english": "Checks if your average is different from a target.",
          "when_to_use": "Use for mean comparisons when σ is unknown.",
          "tags": [
            "test"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "test",
          "output_keys": [
            "t_test_one_sample"
          ],
          "improvement_goal": "Use this test to validate whether a change is real.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "validate",
            "levers": [
              "check assumptions",
              "report effect size + confidence interval",
              "predefine hypotheses",
              "confirm with new data"
            ],
            "warnings": [
              "Statistical significance alone is not practical significance",
              "Multiple testing inflates false positives"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "two-sample-t-test",
          "title": "Two-sample t-test",
          "definition": "Test whether two independent group means differ (with equal/unequal variance variants).",
          "plain_english": "Checks if two groups have different averages.",
          "when_to_use": "Use for group comparisons like before/after (independent samples).",
          "tags": [
            "test"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "test",
          "output_keys": [
            "t_test_two_sample"
          ],
          "improvement_goal": "Use this test to validate whether a change is real.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "validate",
            "levers": [
              "check assumptions",
              "report effect size + confidence interval",
              "predefine hypotheses",
              "confirm with new data"
            ],
            "warnings": [
              "Statistical significance alone is not practical significance",
              "Multiple testing inflates false positives"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "paired-t-test",
          "title": "Paired t-test",
          "definition": "Test of mean difference for paired observations (same unit measured twice).",
          "plain_english": "Checks if individuals changed over time.",
          "when_to_use": "Use for before/after on the same subjects.",
          "tags": [
            "test"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "test",
          "output_keys": [
            "paired_t_test"
          ],
          "improvement_goal": "Use this test to validate whether a change is real.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "validate",
            "levers": [
              "check assumptions",
              "report effect size + confidence interval",
              "predefine hypotheses",
              "confirm with new data"
            ],
            "warnings": [
              "Statistical significance alone is not practical significance",
              "Multiple testing inflates false positives"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "chi-square-test",
          "title": "Chi-Square Test",
          "definition": "Test of association between categorical variables using contingency tables.",
          "plain_english": "Checks if categories are related.",
          "when_to_use": "Use for independence tests on counts (e.g., outcome by group).",
          "tags": [
            "test"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "test",
          "output_keys": [
            "chi_square"
          ],
          "improvement_goal": "Use this test to validate whether a change is real.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "validate",
            "levers": [
              "check assumptions",
              "report effect size + confidence interval",
              "predefine hypotheses",
              "confirm with new data"
            ],
            "warnings": [
              "Statistical significance alone is not practical significance",
              "Multiple testing inflates false positives"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "fishers-exact-test",
          "title": "Fisher’s Exact Test",
          "definition": "Exact test for association in small-sample contingency tables.",
          "plain_english": "A chi-square alternative when counts are small.",
          "when_to_use": "Use when expected cell counts are low.",
          "tags": [
            "test"
          ],
          "level": "advanced",
          "status": "published",
          "quality_score": 80,
          "concept_type": "test",
          "output_keys": [
            "fishers_exact"
          ],
          "improvement_goal": "Use this test to validate whether a change is real.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "validate",
            "levers": [
              "check assumptions",
              "report effect size + confidence interval",
              "predefine hypotheses",
              "confirm with new data"
            ],
            "warnings": [
              "Statistical significance alone is not practical significance",
              "Multiple testing inflates false positives"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "anova-one-way",
          "title": "One-way ANOVA",
          "definition": "Test whether three or more group means differ by comparing between-group to within-group variation.",
          "plain_english": "Checks if at least one group mean is different.",
          "when_to_use": "Use for comparing averages across multiple groups.",
          "tags": [
            "test"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "test",
          "output_keys": [
            "anova_one_way"
          ],
          "improvement_goal": "Use this test to validate whether a change is real.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "validate",
            "levers": [
              "check assumptions",
              "report effect size + confidence interval",
              "predefine hypotheses",
              "confirm with new data"
            ],
            "warnings": [
              "Statistical significance alone is not practical significance",
              "Multiple testing inflates false positives"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "post-hoc-tests",
          "title": "Post-hoc Tests",
          "definition": "Follow-up comparisons after ANOVA to identify which groups differ while controlling error rates.",
          "plain_english": "Finds which groups differ after you know a difference exists.",
          "when_to_use": "Use after ANOVA when you need pairwise differences.",
          "tags": [
            "testing",
            "procedure"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "tukey",
            "bonferroni",
            "post_hoc"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        }
      ]
    },
    {
      "slug": "correlation-relationships",
      "title": "Correlation & Relationships",
      "concepts": [
        {
          "slug": "covariance",
          "title": "Covariance",
          "definition": "Measure of how two variables vary together; sign indicates direction of joint movement.",
          "plain_english": "Do they move together up/down?",
          "when_to_use": "Use as a building block for correlation and multivariate stats.",
          "tags": [
            "relationship"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "covariance"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "pearson-correlation",
          "title": "Pearson Correlation",
          "definition": "Correlation measuring linear association between two continuous variables (−1 to 1).",
          "plain_english": "How strongly two numbers move together in a straight-line way.",
          "when_to_use": "Use for linear relationships with roughly normal data and few outliers.",
          "tags": [
            "relationship"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "pearson_r",
            "correlation"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Variables are measured on compatible scales and aligned in time; spurious correlation is considered."
          ],
          "limitations": [
            "Correlation does not imply causation; confounding and reverse causality are possible."
          ]
        },
        {
          "slug": "spearman-correlation",
          "title": "Spearman Correlation",
          "definition": "Rank-based correlation measuring monotonic association; robust to nonlinearity and outliers.",
          "plain_english": "Whether one tends to increase when the other increases (not necessarily linearly).",
          "when_to_use": "Use for ordinal data or non-normal relationships.",
          "tags": [
            "relationship",
            "robust"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "spearman_r"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Variables are measured on compatible scales and aligned in time; spurious correlation is considered."
          ],
          "limitations": [
            "Correlation does not imply causation; confounding and reverse causality are possible."
          ]
        },
        {
          "slug": "kendall-tau",
          "title": "Kendall’s Tau",
          "definition": "Rank correlation based on concordant/discordant pairs; robust for small samples and ties.",
          "plain_english": "A stable way to compare ranked relationships.",
          "when_to_use": "Use for ordinal data with ties or small n.",
          "tags": [
            "relationship"
          ],
          "level": "advanced",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "kendall_tau"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "correlation-vs-causation",
          "title": "Correlation vs Causation",
          "definition": "Correlation indicates association, not necessarily cause; confounding and reverse causality can mislead.",
          "plain_english": "Two things moving together doesn’t prove one causes the other.",
          "when_to_use": "Use to avoid incorrect causal claims from observational data.",
          "tags": [
            "causality"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Variables are measured on compatible scales and aligned in time; spurious correlation is considered."
          ],
          "limitations": [
            "Correlation does not imply causation; confounding and reverse causality are possible."
          ]
        },
        {
          "slug": "multicollinearity",
          "title": "Multicollinearity",
          "definition": "High correlation among predictors that inflates uncertainty in regression coefficient estimates.",
          "plain_english": "Predictors overlap, making it hard to tell who drives what.",
          "when_to_use": "Use VIF/diagnostics to decide feature selection or regularization.",
          "tags": [
            "regression",
            "diagnostic"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "diagnostic",
          "output_keys": [
            "multicollinearity",
            "vif"
          ],
          "improvement_goal": "Use this diagnostic to confirm model/process assumptions.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "diagnose",
            "levers": [
              "visualize residuals/errors",
              "segment diagnostics by subgroup",
              "address violations (transformations/robust methods)",
              "re-specify model"
            ],
            "warnings": [
              "Ignoring diagnostics leads to biased conclusions"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation.",
            "Relationship is adequately captured by the chosen model form (linear/logit/etc.).",
            "Residuals are not strongly autocorrelated; check diagnostics if time-ordered data."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "partial-correlation",
          "title": "Partial Correlation",
          "definition": "Correlation between two variables after controlling for one or more other variables.",
          "plain_english": "Relationship between A and B after accounting for C.",
          "when_to_use": "Use to reduce confounding in exploratory analysis.",
          "tags": [
            "relationship",
            "control"
          ],
          "level": "advanced",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "partial_correlation"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Variables are measured on compatible scales and aligned in time; spurious correlation is considered."
          ],
          "limitations": [
            "Correlation does not imply causation; confounding and reverse causality are possible."
          ]
        }
      ]
    },
    {
      "slug": "regression",
      "title": "Regression",
      "concepts": [
        {
          "slug": "simple-linear-regression",
          "title": "Simple Linear Regression",
          "definition": "Model predicting a continuous outcome using one predictor with a linear relationship.",
          "plain_english": "A straight-line model for prediction.",
          "when_to_use": "Use to estimate and predict with one main driver.",
          "tags": [
            "regression"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "model",
          "output_keys": [
            "linear_regression_simple",
            "ols"
          ],
          "improvement_goal": "Use this model to predict outcomes and understand drivers.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "improve",
            "levers": [
              "improve features",
              "use proper validation (CV/holdout)",
              "reduce leakage",
              "monitor drift and calibration"
            ],
            "warnings": [
              "Good performance on training data can be misleading",
              "Predictive accuracy does not imply causality"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation.",
            "Relationship is adequately captured by the chosen model form (linear/logit/etc.).",
            "Residuals are not strongly autocorrelated; check diagnostics if time-ordered data."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "multiple-linear-regression",
          "title": "Multiple Linear Regression",
          "definition": "Linear model using multiple predictors to explain/predict a continuous outcome.",
          "plain_english": "A straight-line model with multiple drivers.",
          "when_to_use": "Use when outcomes depend on several factors.",
          "tags": [
            "regression"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "model",
          "output_keys": [
            "linear_regression_multiple",
            "ols"
          ],
          "improvement_goal": "Use this model to predict outcomes and understand drivers.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "improve",
            "levers": [
              "improve features",
              "use proper validation (CV/holdout)",
              "reduce leakage",
              "monitor drift and calibration"
            ],
            "warnings": [
              "Good performance on training data can be misleading",
              "Predictive accuracy does not imply causality"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation.",
            "Relationship is adequately captured by the chosen model form (linear/logit/etc.).",
            "Residuals are not strongly autocorrelated; check diagnostics if time-ordered data."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "regression-assumptions",
          "title": "Regression Assumptions",
          "definition": "Key assumptions include linearity, independent errors, constant variance, and approximately normal residuals for inference.",
          "plain_english": "Rules that must mostly hold for trustworthy coefficients and p-values.",
          "when_to_use": "Use before trusting regression inference and intervals.",
          "tags": [
            "regression"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "assumption",
          "output_keys": [
            "assumptions"
          ],
          "improvement_goal": "Verify this assumption before trusting results.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "verify",
            "levers": [
              "run diagnostics",
              "use robust/nonparametric alternatives if violated",
              "collect better data if needed"
            ],
            "warnings": [
              "If assumptions fail, conclusions may be wrong"
            ]
          },
          "assumptions": [
            "Relationship is adequately captured by the chosen model form (linear/logit/etc.).",
            "Residuals are not strongly autocorrelated; check diagnostics if time-ordered data."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "r-squared",
          "title": "R² and Adjusted R²",
          "definition": "R² is the fraction of outcome variance explained; adjusted R² penalizes unnecessary predictors.",
          "plain_english": "How much your model explains (with a penalty for extra variables).",
          "when_to_use": "Use to compare models (with validation) and explainability.",
          "tags": [
            "regression",
            "metric"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "r_squared",
            "r2",
            "adjusted_r_squared"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Relationship is adequately captured by the chosen model form (linear/logit/etc.).",
            "Residuals are not strongly autocorrelated; check diagnostics if time-ordered data."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "residual-analysis",
          "title": "Residual Analysis",
          "definition": "Examining residuals to check patterns, bias, nonlinearity, and assumption violations.",
          "plain_english": "Look at what the model gets wrong.",
          "when_to_use": "Use to improve model specification and trustworthiness.",
          "tags": [
            "regression",
            "diagnostic"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "diagnostic",
          "output_keys": [
            "residuals"
          ],
          "improvement_goal": "Use this diagnostic to confirm model/process assumptions.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "diagnose",
            "levers": [
              "visualize residuals/errors",
              "segment diagnostics by subgroup",
              "address violations (transformations/robust methods)",
              "re-specify model"
            ],
            "warnings": [
              "Ignoring diagnostics leads to biased conclusions"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation.",
            "Relationship is adequately captured by the chosen model form (linear/logit/etc.).",
            "Residuals are not strongly autocorrelated; check diagnostics if time-ordered data."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "heteroscedasticity",
          "title": "Heteroscedasticity",
          "definition": "Non-constant residual variance across fitted values or predictors, affecting standard errors.",
          "plain_english": "Errors spread more for some values than others.",
          "when_to_use": "Use robust SEs, transformations, or re-specify the model if present.",
          "tags": [
            "regression",
            "diagnostic"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "diagnostic",
          "output_keys": [
            "heteroscedasticity",
            "breusch_pagan"
          ],
          "improvement_goal": "Use this diagnostic to confirm model/process assumptions.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "diagnose",
            "levers": [
              "visualize residuals/errors",
              "segment diagnostics by subgroup",
              "address violations (transformations/robust methods)",
              "re-specify model"
            ],
            "warnings": [
              "Ignoring diagnostics leads to biased conclusions"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation.",
            "Relationship is adequately captured by the chosen model form (linear/logit/etc.).",
            "Residuals are not strongly autocorrelated; check diagnostics if time-ordered data."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "autocorrelation",
          "title": "Autocorrelation",
          "definition": "Correlation of residuals over time/order, violating independence and biasing inference.",
          "plain_english": "Errors are related across time steps.",
          "when_to_use": "Use time-series methods or add lag terms if present.",
          "tags": [
            "regression",
            "diagnostic"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "diagnostic",
          "output_keys": [
            "autocorrelation",
            "durbin_watson"
          ],
          "improvement_goal": "Use this diagnostic to confirm model/process assumptions.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "diagnose",
            "levers": [
              "visualize residuals/errors",
              "segment diagnostics by subgroup",
              "address violations (transformations/robust methods)",
              "re-specify model"
            ],
            "warnings": [
              "Ignoring diagnostics leads to biased conclusions"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation.",
            "Relationship is adequately captured by the chosen model form (linear/logit/etc.).",
            "Residuals are not strongly autocorrelated; check diagnostics if time-ordered data.",
            "Variables are measured on compatible scales and aligned in time; spurious correlation is considered."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives.",
            "Correlation does not imply causation; confounding and reverse causality are possible."
          ]
        },
        {
          "slug": "polynomial-regression",
          "title": "Polynomial Regression",
          "definition": "Regression using polynomial terms to model curved relationships.",
          "plain_english": "A curved line model.",
          "when_to_use": "Use when relationship is nonlinear but smooth.",
          "tags": [
            "regression"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "model",
          "output_keys": [
            "polynomial_regression"
          ],
          "improvement_goal": "Use this model to predict outcomes and understand drivers.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "improve",
            "levers": [
              "improve features",
              "use proper validation (CV/holdout)",
              "reduce leakage",
              "monitor drift and calibration"
            ],
            "warnings": [
              "Good performance on training data can be misleading",
              "Predictive accuracy does not imply causality"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation.",
            "Relationship is adequately captured by the chosen model form (linear/logit/etc.).",
            "Residuals are not strongly autocorrelated; check diagnostics if time-ordered data."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "interaction-terms",
          "title": "Interaction Terms",
          "definition": "Terms that allow the effect of one predictor to depend on another predictor.",
          "plain_english": "One factor changes how another factor matters.",
          "when_to_use": "Use when effects differ across subgroups or conditions.",
          "tags": [
            "regression"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "interaction_terms"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation.",
            "Relationship is adequately captured by the chosen model form (linear/logit/etc.).",
            "Residuals are not strongly autocorrelated; check diagnostics if time-ordered data."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "dummy-variables",
          "title": "Dummy Variables (One-hot)",
          "definition": "Binary indicators representing categories for use in regression and ML models.",
          "plain_english": "Turn categories into 0/1 columns.",
          "when_to_use": "Use when modeling categorical predictors.",
          "tags": [
            "preprocessing"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "dummy_variables",
            "one_hot"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "coefficient-interpretation",
          "title": "Coefficient Interpretation",
          "definition": "Interpreting coefficients as expected change in outcome per unit change in predictor (holding others constant).",
          "plain_english": "What each driver means in the model.",
          "when_to_use": "Use to translate models into business/clinical insight.",
          "tags": [
            "regression"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "coefficients",
            "betas"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation.",
            "Relationship is adequately captured by the chosen model form (linear/logit/etc.).",
            "Residuals are not strongly autocorrelated; check diagnostics if time-ordered data."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "ci-for-coefficients",
          "title": "CI for Coefficients",
          "definition": "Confidence intervals for coefficients quantify uncertainty around estimated effects.",
          "plain_english": "A plausible range for each driver’s impact.",
          "when_to_use": "Use to judge stability and practical significance of coefficients.",
          "tags": [
            "regression"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "coef_ci",
            "confidence_interval"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation.",
            "Relationship is adequately captured by the chosen model form (linear/logit/etc.).",
            "Residuals are not strongly autocorrelated; check diagnostics if time-ordered data."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "hypothesis-tests-for-betas",
          "title": "Hypothesis Tests for Betas",
          "definition": "Tests whether each coefficient differs from zero (or another value) using t-statistics and p-values.",
          "plain_english": "Checks if a driver likely matters beyond noise.",
          "when_to_use": "Use to screen drivers (with caution about multiple testing).",
          "tags": [
            "regression",
            "testing"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "test",
          "output_keys": [
            "beta_tests",
            "t_stat",
            "p_value"
          ],
          "improvement_goal": "Use this test to validate whether a change is real.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "validate",
            "levers": [
              "check assumptions",
              "report effect size + confidence interval",
              "predefine hypotheses",
              "confirm with new data"
            ],
            "warnings": [
              "Statistical significance alone is not practical significance",
              "Multiple testing inflates false positives"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation.",
            "Relationship is adequately captured by the chosen model form (linear/logit/etc.).",
            "Residuals are not strongly autocorrelated; check diagnostics if time-ordered data."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "regularization-ridge",
          "title": "Ridge Regression",
          "definition": "Regression with L2 penalty that shrinks coefficients to reduce overfitting and handle collinearity.",
          "plain_english": "A model that reduces overreaction to noisy predictors.",
          "when_to_use": "Use when many correlated predictors exist.",
          "tags": [
            "regularization"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "model",
          "output_keys": [
            "ridge"
          ],
          "improvement_goal": "Use this model to predict outcomes and understand drivers.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "improve",
            "levers": [
              "improve features",
              "use proper validation (CV/holdout)",
              "reduce leakage",
              "monitor drift and calibration"
            ],
            "warnings": [
              "Good performance on training data can be misleading",
              "Predictive accuracy does not imply causality"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation.",
            "Relationship is adequately captured by the chosen model form (linear/logit/etc.).",
            "Residuals are not strongly autocorrelated; check diagnostics if time-ordered data."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "regularization-lasso",
          "title": "Lasso Regression",
          "definition": "Regression with L1 penalty that can set some coefficients to zero (feature selection).",
          "plain_english": "A model that can drop unnecessary predictors.",
          "when_to_use": "Use for sparse models and feature selection.",
          "tags": [
            "regularization"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "model",
          "output_keys": [
            "lasso"
          ],
          "improvement_goal": "Use this model to predict outcomes and understand drivers.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "improve",
            "levers": [
              "improve features",
              "use proper validation (CV/holdout)",
              "reduce leakage",
              "monitor drift and calibration"
            ],
            "warnings": [
              "Good performance on training data can be misleading",
              "Predictive accuracy does not imply causality"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation.",
            "Relationship is adequately captured by the chosen model form (linear/logit/etc.).",
            "Residuals are not strongly autocorrelated; check diagnostics if time-ordered data."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "elastic-net",
          "title": "Elastic Net",
          "definition": "Regularization combining L1 and L2 penalties, balancing shrinkage and selection.",
          "plain_english": "Mixes ridge and lasso benefits.",
          "when_to_use": "Use with many correlated features and desire for sparsity.",
          "tags": [
            "regularization"
          ],
          "level": "advanced",
          "status": "published",
          "quality_score": 80,
          "concept_type": "model",
          "output_keys": [
            "elastic_net"
          ],
          "improvement_goal": "Use this model to predict outcomes and understand drivers.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "improve",
            "levers": [
              "improve features",
              "use proper validation (CV/holdout)",
              "reduce leakage",
              "monitor drift and calibration"
            ],
            "warnings": [
              "Good performance on training data can be misleading",
              "Predictive accuracy does not imply causality"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "transformations-log",
          "title": "Log Transformation",
          "definition": "Applying log to reduce skew, stabilize variance, and linearize multiplicative relationships.",
          "plain_english": "Compresses big values and tames long tails.",
          "when_to_use": "Use when data are right-skewed or variance grows with the mean.",
          "tags": [
            "transformations"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "log_transform"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "box-cox",
          "title": "Box-Cox Transformation",
          "definition": "Family of power transforms chosen to improve normality/variance stability.",
          "plain_english": "A smart transformation chooser for skewed data.",
          "when_to_use": "Use when you need a transformation and want a data-driven choice.",
          "tags": [
            "transformations"
          ],
          "level": "advanced",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "box_cox"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        }
      ]
    },
    {
      "slug": "advanced-methods",
      "title": "Advanced Methods",
      "concepts": [
        {
          "slug": "logistic-regression",
          "title": "Logistic Regression",
          "definition": "Classification model predicting probability of a binary outcome using a logistic link function.",
          "plain_english": "Predicts yes/no probability.",
          "when_to_use": "Use for binary outcomes like readmission (yes/no), defect (yes/no).",
          "tags": [
            "classification"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "model",
          "output_keys": [
            "logistic_regression"
          ],
          "improvement_goal": "Use this model to predict outcomes and understand drivers.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "improve",
            "levers": [
              "improve features",
              "use proper validation (CV/holdout)",
              "reduce leakage",
              "monitor drift and calibration"
            ],
            "warnings": [
              "Good performance on training data can be misleading",
              "Predictive accuracy does not imply causality"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation.",
            "Relationship is adequately captured by the chosen model form (linear/logit/etc.).",
            "Residuals are not strongly autocorrelated; check diagnostics if time-ordered data."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "odds-ratios",
          "title": "Odds Ratios",
          "definition": "Ratio of odds; in logistic regression, exp(coef) is the odds ratio for a 1-unit predictor change.",
          "plain_english": "How much the odds change when a driver changes.",
          "when_to_use": "Use to interpret logistic regression effects.",
          "tags": [
            "classification"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "odds_ratio",
            "or"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "maximum-likelihood-estimation",
          "title": "Maximum Likelihood Estimation (MLE)",
          "definition": "Parameter estimation method choosing values that maximize the likelihood of observed data under a model.",
          "plain_english": "Finds the parameters that make the data most plausible.",
          "when_to_use": "Use in many models (GLM, logistic regression) and distribution fitting.",
          "tags": [
            "estimation"
          ],
          "level": "advanced",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "mle"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "generalized-linear-models",
          "title": "Generalized Linear Models (GLM)",
          "definition": "Extension of linear regression allowing non-normal outcomes via link functions (e.g., logistic, Poisson).",
          "plain_english": "Linear models for many outcome types.",
          "when_to_use": "Use for counts, proportions, and other non-normal outcomes.",
          "tags": [
            "modeling"
          ],
          "level": "advanced",
          "status": "published",
          "quality_score": 80,
          "concept_type": "model",
          "output_keys": [
            "glm"
          ],
          "improvement_goal": "Use this model to predict outcomes and understand drivers.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "improve",
            "levers": [
              "improve features",
              "use proper validation (CV/holdout)",
              "reduce leakage",
              "monitor drift and calibration"
            ],
            "warnings": [
              "Good performance on training data can be misleading",
              "Predictive accuracy does not imply causality"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "nonparametric-tests",
          "title": "Nonparametric Tests",
          "definition": "Tests that rely less on distributional assumptions, often using ranks.",
          "plain_english": "Tests that work even when data aren’t normal.",
          "when_to_use": "Use when normality/variance assumptions fail or for ordinal data.",
          "tags": [
            "testing",
            "robust"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "test",
          "output_keys": [
            "mann_whitney",
            "kruskal_wallis"
          ],
          "improvement_goal": "Use this test to validate whether a change is real.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "validate",
            "levers": [
              "check assumptions",
              "report effect size + confidence interval",
              "predefine hypotheses",
              "confirm with new data"
            ],
            "warnings": [
              "Statistical significance alone is not practical significance",
              "Multiple testing inflates false positives"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "bootstrapping",
          "title": "Bootstrapping",
          "definition": "Resampling method to estimate uncertainty by repeatedly sampling with replacement from the observed data.",
          "plain_english": "Simulates many 'new samples' from your data.",
          "when_to_use": "Use for confidence intervals when formulas are hard or assumptions are weak.",
          "tags": [
            "resampling"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "bootstrap"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "permutation-tests",
          "title": "Permutation Tests",
          "definition": "Tests significance by comparing the observed statistic to values obtained by randomly permuting labels.",
          "plain_english": "Checks how unusual your result is under 'no effect' by shuffling.",
          "when_to_use": "Use when assumptions are unclear and you can shuffle labels.",
          "tags": [
            "testing",
            "resampling"
          ],
          "level": "advanced",
          "status": "published",
          "quality_score": 80,
          "concept_type": "test",
          "output_keys": [
            "permutation_test"
          ],
          "improvement_goal": "Use this test to validate whether a change is real.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "validate",
            "levers": [
              "check assumptions",
              "report effect size + confidence interval",
              "predefine hypotheses",
              "confirm with new data"
            ],
            "warnings": [
              "Statistical significance alone is not practical significance",
              "Multiple testing inflates false positives"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "bayesian-statistics-intro",
          "title": "Bayesian Statistics (Intro)",
          "definition": "Framework that updates prior beliefs with data to produce a posterior distribution.",
          "plain_english": "Start with a belief, then update it with evidence.",
          "when_to_use": "Use when you want probabilistic statements and to incorporate prior knowledge.",
          "tags": [
            "bayesian"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [
            "bayesian"
          ],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "prior-posterior-likelihood",
          "title": "Prior / Posterior / Likelihood",
          "definition": "Prior encodes beliefs before data; likelihood measures data fit; posterior combines them via Bayes’ rule.",
          "plain_english": "Before-data belief + evidence = after-data belief.",
          "when_to_use": "Use to interpret Bayesian outputs clearly.",
          "tags": [
            "bayesian"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [
            "prior",
            "posterior",
            "likelihood"
          ],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "quantile-regression",
          "title": "Quantile Regression",
          "definition": "Regression modeling conditional quantiles (e.g., median, 90th percentile) instead of the mean.",
          "plain_english": "Predicts the median or p90, not the average.",
          "when_to_use": "Use for tail-focused goals (reduce p90 wait time).",
          "tags": [
            "regression"
          ],
          "level": "advanced",
          "status": "published",
          "quality_score": 80,
          "concept_type": "model",
          "output_keys": [
            "quantile_regression"
          ],
          "improvement_goal": "Use this model to predict outcomes and understand drivers.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "improve",
            "levers": [
              "improve features",
              "use proper validation (CV/holdout)",
              "reduce leakage",
              "monitor drift and calibration"
            ],
            "warnings": [
              "Good performance on training data can be misleading",
              "Predictive accuracy does not imply causality"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation.",
            "Relationship is adequately captured by the chosen model form (linear/logit/etc.).",
            "Residuals are not strongly autocorrelated; check diagnostics if time-ordered data."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "principal-component-analysis",
          "title": "Principal Component Analysis (PCA)",
          "definition": "Dimensionality reduction that transforms correlated variables into uncorrelated components capturing most variance.",
          "plain_english": "Compresses many correlated features into a few summary features.",
          "when_to_use": "Use for visualization, noise reduction, and multicollinearity handling.",
          "tags": [
            "dimension-reduction"
          ],
          "level": "advanced",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "pca"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "k-means-clustering",
          "title": "K-means Clustering",
          "definition": "Unsupervised method that partitions data into k clusters by minimizing within-cluster distances.",
          "plain_english": "Groups similar rows together automatically.",
          "when_to_use": "Use to segment populations or identify patterns without labels.",
          "tags": [
            "unsupervised"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "model",
          "output_keys": [
            "kmeans",
            "k_means"
          ],
          "improvement_goal": "Use this model to predict outcomes and understand drivers.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "improve",
            "levers": [
              "improve features",
              "use proper validation (CV/holdout)",
              "reduce leakage",
              "monitor drift and calibration"
            ],
            "warnings": [
              "Good performance on training data can be misleading",
              "Predictive accuracy does not imply causality"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "silhouette-score",
          "title": "Silhouette Score",
          "definition": "Metric assessing clustering quality based on separation between clusters and cohesion within clusters.",
          "plain_english": "How well your clusters are separated and consistent.",
          "when_to_use": "Use to evaluate and choose number of clusters.",
          "tags": [
            "unsupervised"
          ],
          "level": "advanced",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "silhouette_score"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        }
      ]
    },
    {
      "slug": "time-series",
      "title": "Time Series & Forecasting",
      "concepts": [
        {
          "slug": "time-series-components",
          "title": "Time Series Components",
          "definition": "Time series often include trend, seasonality, cycles, and irregular noise components.",
          "plain_english": "Time data usually has patterns plus randomness.",
          "when_to_use": "Use to decide forecasting methods and feature engineering.",
          "tags": [
            "time-series"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [
            "trend",
            "seasonality",
            "noise"
          ],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Historical patterns (trend/seasonality) are relevant for the forecast horizon; structural breaks reduce accuracy.",
            "Missing timestamps are handled; regular time spacing is ensured or modeled."
          ],
          "limitations": [
            "Forecast error grows with horizon; sudden process changes can invalidate models."
          ]
        },
        {
          "slug": "decomposition",
          "title": "Time Series Decomposition",
          "definition": "Technique that separates a series into trend, seasonal, and residual components.",
          "plain_english": "Splits the series into understandable parts.",
          "when_to_use": "Use to diagnose seasonality and underlying trends.",
          "tags": [
            "time-series"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "decomposition"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation.",
            "Historical patterns (trend/seasonality) are relevant for the forecast horizon; structural breaks reduce accuracy.",
            "Missing timestamps are handled; regular time spacing is ensured or modeled."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives.",
            "Forecast error grows with horizon; sudden process changes can invalidate models."
          ]
        },
        {
          "slug": "stationarity",
          "title": "Stationarity",
          "definition": "A stationary series has stable statistical properties over time (mean/variance/autocorrelation).",
          "plain_english": "Its behavior doesn’t drift unpredictably over time.",
          "when_to_use": "Use because many models (ARIMA) assume stationarity.",
          "tags": [
            "time-series"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [
            "stationarity"
          ],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "acf",
          "title": "Autocorrelation Function (ACF)",
          "definition": "Correlation of a series with its lagged values across different lags.",
          "plain_english": "How today relates to yesterday/last week.",
          "when_to_use": "Use to identify persistence and choose ARIMA terms.",
          "tags": [
            "time-series"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "diagnostic",
          "output_keys": [
            "acf"
          ],
          "improvement_goal": "Use this diagnostic to confirm model/process assumptions.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "diagnose",
            "levers": [
              "visualize residuals/errors",
              "segment diagnostics by subgroup",
              "address violations (transformations/robust methods)",
              "re-specify model"
            ],
            "warnings": [
              "Ignoring diagnostics leads to biased conclusions"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives.",
            "Correlation does not imply causation; confounding and reverse causality are possible."
          ]
        },
        {
          "slug": "pacf",
          "title": "Partial Autocorrelation (PACF)",
          "definition": "Correlation between series and lag after removing effects of shorter lags.",
          "plain_english": "Direct lag relationship after accounting for earlier lags.",
          "when_to_use": "Use to select AR order in ARIMA.",
          "tags": [
            "time-series"
          ],
          "level": "advanced",
          "status": "published",
          "quality_score": 80,
          "concept_type": "diagnostic",
          "output_keys": [
            "pacf"
          ],
          "improvement_goal": "Use this diagnostic to confirm model/process assumptions.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "diagnose",
            "levers": [
              "visualize residuals/errors",
              "segment diagnostics by subgroup",
              "address violations (transformations/robust methods)",
              "re-specify model"
            ],
            "warnings": [
              "Ignoring diagnostics leads to biased conclusions"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives.",
            "Correlation does not imply causation; confounding and reverse causality are possible."
          ]
        },
        {
          "slug": "moving-averages",
          "title": "Moving Averages",
          "definition": "Rolling average that smooths short-term fluctuations.",
          "plain_english": "A smoothed version of your trend.",
          "when_to_use": "Use for smoothing and baseline forecasting.",
          "tags": [
            "time-series"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "moving_average"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "exponential-smoothing",
          "title": "Exponential Smoothing",
          "definition": "Forecasting method weighting recent observations more heavily, with variants for trend/seasonality.",
          "plain_english": "Recent data matters more than old data.",
          "when_to_use": "Use for short-term forecasting with stable patterns.",
          "tags": [
            "time-series"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "exponential_smoothing"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "arima",
          "title": "ARIMA",
          "definition": "Autoregressive Integrated Moving Average model for forecasting stationary series (after differencing).",
          "plain_english": "A classic forecasting model using past values and errors.",
          "when_to_use": "Use for time series with autocorrelation and limited features.",
          "tags": [
            "time-series"
          ],
          "level": "advanced",
          "status": "published",
          "quality_score": 80,
          "concept_type": "model",
          "output_keys": [
            "arima"
          ],
          "improvement_goal": "Use this model to predict outcomes and understand drivers.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "improve",
            "levers": [
              "improve features",
              "use proper validation (CV/holdout)",
              "reduce leakage",
              "monitor drift and calibration"
            ],
            "warnings": [
              "Good performance on training data can be misleading",
              "Predictive accuracy does not imply causality"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation.",
            "Historical patterns (trend/seasonality) are relevant for the forecast horizon; structural breaks reduce accuracy.",
            "Missing timestamps are handled; regular time spacing is ensured or modeled."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "sarima",
          "title": "SARIMA",
          "definition": "ARIMA extended with seasonal terms to model repeating seasonal patterns.",
          "plain_english": "ARIMA that handles seasonality.",
          "when_to_use": "Use when seasonality is present.",
          "tags": [
            "time-series"
          ],
          "level": "advanced",
          "status": "published",
          "quality_score": 80,
          "concept_type": "model",
          "output_keys": [
            "sarima"
          ],
          "improvement_goal": "Use this model to predict outcomes and understand drivers.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "improve",
            "levers": [
              "improve features",
              "use proper validation (CV/holdout)",
              "reduce leakage",
              "monitor drift and calibration"
            ],
            "warnings": [
              "Good performance on training data can be misleading",
              "Predictive accuracy does not imply causality"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation.",
            "Historical patterns (trend/seasonality) are relevant for the forecast horizon; structural breaks reduce accuracy.",
            "Missing timestamps are handled; regular time spacing is ensured or modeled."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "prophet",
          "title": "Prophet (Conceptual)",
          "definition": "Forecasting approach combining trend + seasonality + holidays with robust fitting.",
          "plain_english": "A practical forecasting tool for business seasonality.",
          "when_to_use": "Use for business time series with multiple seasonalities and events.",
          "tags": [
            "time-series"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "model",
          "output_keys": [
            "prophet"
          ],
          "improvement_goal": "Use this model to predict outcomes and understand drivers.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "improve",
            "levers": [
              "improve features",
              "use proper validation (CV/holdout)",
              "reduce leakage",
              "monitor drift and calibration"
            ],
            "warnings": [
              "Good performance on training data can be misleading",
              "Predictive accuracy does not imply causality"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "mape",
          "title": "MAPE",
          "definition": "Mean Absolute Percentage Error: average absolute error as a percentage of actual values.",
          "plain_english": "Average percent error in forecasts.",
          "when_to_use": "Use to communicate forecast accuracy in intuitive terms (watch out for zeros).",
          "tags": [
            "forecasting",
            "metric"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "mape"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        }
      ]
    },
    {
      "slug": "predictive-ml",
      "title": "Predictive Modeling & ML",
      "concepts": [
        {
          "slug": "train-test-split",
          "title": "Train/Test Split",
          "definition": "Partition data into training and testing sets to estimate generalization performance.",
          "plain_english": "Keep some data untouched to test honestly.",
          "when_to_use": "Use before modeling to avoid over-optimism.",
          "tags": [
            "validation"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "train_test_split"
          ],
          "improvement_goal": "Build models you can trust in production.",
          "diagnostic_questions": [
            "Does performance hold on unseen data?",
            "Is there leakage or target contamination?",
            "Does the model stay stable over time and across subgroups?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "cross-validation",
          "title": "Cross-Validation",
          "definition": "Resampling method that trains and evaluates models across multiple folds of the data.",
          "plain_english": "Test the model multiple times on different slices.",
          "when_to_use": "Use for robust performance estimation and model selection.",
          "tags": [
            "validation"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "cross_validation",
            "cv"
          ],
          "improvement_goal": "Build models you can trust in production.",
          "diagnostic_questions": [
            "Does performance hold on unseen data?",
            "Is there leakage or target contamination?",
            "Does the model stay stable over time and across subgroups?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "stratified-cross-validation",
          "title": "Stratified Cross-Validation",
          "definition": "Cross-validation that preserves class proportions in each fold for classification problems.",
          "plain_english": "Keeps positives/negatives balanced in each split.",
          "when_to_use": "Use for imbalanced classification.",
          "tags": [
            "validation"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "stratified_cv"
          ],
          "improvement_goal": "Build models you can trust in production.",
          "diagnostic_questions": [
            "Does performance hold on unseen data?",
            "Is there leakage or target contamination?",
            "Does the model stay stable over time and across subgroups?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "bias-variance-tradeoff",
          "title": "Bias–Variance Tradeoff",
          "definition": "Tradeoff between underfitting (high bias) and overfitting (high variance) affecting generalization.",
          "plain_english": "Too simple misses patterns; too complex memorizes noise.",
          "when_to_use": "Use to choose model complexity and regularization.",
          "tags": [
            "modeling"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "definition",
          "output_keys": [
            "bias_variance"
          ],
          "improvement_goal": "Use this concept to frame the analysis correctly.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "use consistent definitions",
              "align stakeholders on meaning",
              "document terms and units"
            ],
            "warnings": [
              "Ambiguity causes misinterpretation"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "feature-engineering",
          "title": "Feature Engineering",
          "definition": "Creating or transforming variables to improve model performance and interpretability.",
          "plain_english": "Design better inputs for the model.",
          "when_to_use": "Use to capture domain logic and reduce noise.",
          "tags": [
            "features"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "feature_engineering"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "feature-selection",
          "title": "Feature Selection",
          "definition": "Selecting a subset of useful predictors to reduce overfitting and improve interpretability.",
          "plain_english": "Keep the drivers that matter most.",
          "when_to_use": "Use with many features or collinearity.",
          "tags": [
            "features"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "feature_selection"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "hyperparameter-tuning",
          "title": "Hyperparameter Tuning",
          "definition": "Searching model settings (not learned from data) to improve performance under validation.",
          "plain_english": "Try different settings to find what works best.",
          "when_to_use": "Use with CV and a defined metric to avoid overfitting to the test set.",
          "tags": [
            "modeling"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "procedure",
          "output_keys": [
            "hyperparameter_tuning",
            "grid_search",
            "random_search"
          ],
          "improvement_goal": "Apply this procedure to run analysis consistently.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "apply",
            "levers": [
              "use a checklist",
              "document assumptions",
              "automate repeatable steps",
              "use guardrails for edge cases"
            ],
            "warnings": [
              "Inconsistent procedures cause inconsistent results"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "learning-curves",
          "title": "Learning Curves",
          "definition": "Plots of performance vs training set size to diagnose data sufficiency and bias/variance issues.",
          "plain_english": "Shows whether more data will help.",
          "when_to_use": "Use to decide whether to collect more data or change the model.",
          "tags": [
            "modeling",
            "diagnostic"
          ],
          "level": "advanced",
          "status": "published",
          "quality_score": 80,
          "concept_type": "chart",
          "output_keys": [
            "learning_curve"
          ],
          "improvement_goal": "Use this visualization to spot patterns and variation.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "clarify",
            "levers": [
              "choose the right chart for the question",
              "label axes/units",
              "show distribution (not only averages)",
              "avoid misleading scales"
            ],
            "warnings": [
              "Bad visuals create bad decisions"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "decision-trees",
          "title": "Decision Trees",
          "definition": "Model that splits data by feature thresholds to predict outcomes.",
          "plain_english": "A flowchart of decisions.",
          "when_to_use": "Use for interpretable nonlinear relationships.",
          "tags": [
            "modeling"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "model",
          "output_keys": [
            "decision_tree"
          ],
          "improvement_goal": "Use this model to predict outcomes and understand drivers.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "improve",
            "levers": [
              "improve features",
              "use proper validation (CV/holdout)",
              "reduce leakage",
              "monitor drift and calibration"
            ],
            "warnings": [
              "Good performance on training data can be misleading",
              "Predictive accuracy does not imply causality"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "random-forest",
          "title": "Random Forest",
          "definition": "Ensemble of decision trees trained on bootstrapped samples and random feature subsets.",
          "plain_english": "Many trees vote to improve accuracy and stability.",
          "when_to_use": "Use for strong baseline performance with tabular data.",
          "tags": [
            "modeling"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "model",
          "output_keys": [
            "random_forest"
          ],
          "improvement_goal": "Use this model to predict outcomes and understand drivers.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "improve",
            "levers": [
              "improve features",
              "use proper validation (CV/holdout)",
              "reduce leakage",
              "monitor drift and calibration"
            ],
            "warnings": [
              "Good performance on training data can be misleading",
              "Predictive accuracy does not imply causality"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "gradient-boosting",
          "title": "Gradient Boosting",
          "definition": "Ensemble method that builds trees sequentially to correct previous errors.",
          "plain_english": "Trees that learn from mistakes.",
          "when_to_use": "Use for high-performance tabular prediction (with careful tuning).",
          "tags": [
            "modeling"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "model",
          "output_keys": [
            "gradient_boosting",
            "gbm"
          ],
          "improvement_goal": "Use this model to predict outcomes and understand drivers.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "improve",
            "levers": [
              "improve features",
              "use proper validation (CV/holdout)",
              "reduce leakage",
              "monitor drift and calibration"
            ],
            "warnings": [
              "Good performance on training data can be misleading",
              "Predictive accuracy does not imply causality"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "xgboost",
          "title": "XGBoost (Conceptual)",
          "definition": "Optimized gradient boosting implementation with regularization and efficient training.",
          "plain_english": "A powerful boosted-tree engine.",
          "when_to_use": "Use when you need top performance on structured data.",
          "tags": [
            "modeling"
          ],
          "level": "advanced",
          "status": "published",
          "quality_score": 80,
          "concept_type": "model",
          "output_keys": [
            "xgboost"
          ],
          "improvement_goal": "Use this model to predict outcomes and understand drivers.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "improve",
            "levers": [
              "improve features",
              "use proper validation (CV/holdout)",
              "reduce leakage",
              "monitor drift and calibration"
            ],
            "warnings": [
              "Good performance on training data can be misleading",
              "Predictive accuracy does not imply causality"
            ]
          },
          "assumptions": [
            "Data is appropriately collected and defined (units, time window, and groups are clear).",
            "Observations are independent unless the method explicitly handles pairing/serial correlation."
          ],
          "limitations": [
            "Statistical significance does not guarantee practical/clinical significance; check effect size and context.",
            "Violating assumptions can produce misleading results; use diagnostics and robust alternatives."
          ]
        },
        {
          "slug": "mae",
          "title": "MAE",
          "definition": "Mean Absolute Error: average absolute difference between predictions and actuals.",
          "plain_english": "On average, how far off you are (in units).",
          "when_to_use": "Use for robust error measurement less sensitive to outliers than MSE.",
          "tags": [
            "metrics"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "mae"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "mse",
          "title": "MSE",
          "definition": "Mean Squared Error: average of squared prediction errors.",
          "plain_english": "Penalizes big mistakes more heavily.",
          "when_to_use": "Use when large errors are especially costly.",
          "tags": [
            "metrics"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "mse"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "rmse",
          "title": "RMSE",
          "definition": "Root Mean Squared Error: square root of MSE, in original units.",
          "plain_english": "Typical size of your error (with emphasis on large misses).",
          "when_to_use": "Use when you want unit-based error comparable across models.",
          "tags": [
            "metrics"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "rmse"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "log-loss",
          "title": "Log Loss",
          "definition": "Loss for probabilistic classification that penalizes confident wrong predictions heavily.",
          "plain_english": "Punishes being confidently wrong.",
          "when_to_use": "Use for evaluating predicted probabilities, not just class labels.",
          "tags": [
            "metrics"
          ],
          "level": "advanced",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "log_loss",
            "cross_entropy"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "brier-score",
          "title": "Brier Score",
          "definition": "Mean squared error of predicted probabilities for binary outcomes.",
          "plain_english": "How accurate and calibrated your probabilities are.",
          "when_to_use": "Use when probability quality matters (risk scores).",
          "tags": [
            "metrics"
          ],
          "level": "advanced",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "brier_score"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "confusion-matrix",
          "title": "Confusion Matrix",
          "definition": "Table of true/false positives/negatives summarizing classification outcomes.",
          "plain_english": "Shows what types of errors your classifier makes.",
          "when_to_use": "Use to understand tradeoffs and costs of mistakes.",
          "tags": [
            "metrics"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "confusion_matrix"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "accuracy",
          "title": "Accuracy",
          "definition": "Proportion of correct predictions.",
          "plain_english": "How often the model is right.",
          "when_to_use": "Use when classes are balanced and error costs are similar.",
          "tags": [
            "metrics"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "accuracy"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "precision",
          "title": "Precision",
          "definition": "Among predicted positives, the proportion that are truly positive.",
          "plain_english": "When the model says 'yes', how often is it correct?",
          "when_to_use": "Use when false positives are costly.",
          "tags": [
            "metrics"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "precision"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "recall",
          "title": "Recall (Sensitivity)",
          "definition": "Among true positives, the proportion the model correctly identifies.",
          "plain_english": "How many real positives you catch.",
          "when_to_use": "Use when missing positives is costly.",
          "tags": [
            "metrics"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "recall",
            "sensitivity"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "specificity",
          "title": "Specificity",
          "definition": "Among true negatives, the proportion correctly identified.",
          "plain_english": "How well you avoid false alarms.",
          "when_to_use": "Use with recall to understand tradeoffs.",
          "tags": [
            "metrics"
          ],
          "level": "intermediate",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "specificity"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        },
        {
          "slug": "f1-score",
          "title": "F1 Score",
          "definition": "Harmonic mean of precision and recall.",
          "plain_english": "A single score balancing misses and false alarms.",
          "when_to_use": "Use for imbalanced classes where both error types matter.",
          "tags": [
            "metrics"
          ],
          "level": "intro",
          "status": "published",
          "quality_score": 80,
          "concept_type": "metric",
          "output_keys": [
            "f1"
          ],
          "improvement_goal": "Measure and improve performance using this metric.",
          "diagnostic_questions": [
            "When does this apply to your question and data?",
            "What decision will you make based on the result?"
          ],
          "improvement_playbook": {
            "direction": "optimize",
            "levers": [
              "define a target and guardrails",
              "segment by unit/time/category",
              "monitor trend and variation",
              "act on the biggest drivers"
            ],
            "warnings": [
              "Do not optimize one metric while harming outcomes that matter more"
            ]
          },
          "assumptions": [
            "Concept is applied to correctly defined data with consistent units and meaning."
          ],
          "limitations": [
            "If inputs are poorly defined or noisy, this concept can be misused or misinterpreted."
          ]
        }
      ]
    },
    {
      "slug": "model-reliability",
      "title": "Model Reliability & Validation",
      "concepts": []
    },
    {
      "slug": "data-quality-prep",
      "title": "Data Quality & Prep",
      "concepts": []
    },
    {
      "slug": "causal-thinking",
      "title": "Causal Thinking",
      "concepts": []
    },
    {
      "slug": "process-improvement",
      "title": "Process & Improvement Stats",
      "concepts": []
    }
  ]
}