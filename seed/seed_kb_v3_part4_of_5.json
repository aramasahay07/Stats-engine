{"version": "v3-high-quality-complete-split", "generated_at": "2025-12-21T00:12:56.537079+00:00", "part": 4, "parts_total": 5, "topics": [{"slug": "time-series", "title": "Time Series & Forecasting", "concepts": [{"slug": "time-series-components", "title": "Time Series Components", "definition": "Time series often include trend, seasonality, cycles, and irregular noise components.", "plain_english": "Time data usually has patterns plus randomness.", "when_to_use": "Use to decide forecasting methods and feature engineering.", "tags": ["time-series"], "level": "intro", "status": "published", "quality_score": 80, "concept_type": "definition", "output_keys": ["trend", "seasonality", "noise"], "improvement_goal": "Use this concept to frame the analysis correctly.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "clarify", "levers": ["use consistent definitions", "align stakeholders on meaning", "document terms and units"], "warnings": ["Ambiguity causes misinterpretation"]}, "assumptions": "Use consistent definitions, units, and measurement processes.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "decomposition", "link_type": "related", "note": "Often learned alongside Time Series Decomposition."}, {"to_slug": "stationarity", "link_type": "related", "note": "Often learned alongside Stationarity."}, {"to_slug": "acf", "link_type": "related", "note": "Often learned alongside Autocorrelation Function (ACF)."}], "formulas": [], "examples": [{"title": "Time Series Components micro example", "scenario": "You are analyzing a dataset and need to apply: Time Series Components.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "decomposition", "title": "Time Series Decomposition", "definition": "Technique that separates a series into trend, seasonal, and residual components.", "plain_english": "Splits the series into understandable parts.", "when_to_use": "Use to diagnose seasonality and underlying trends.", "tags": ["time-series"], "level": "intermediate", "status": "published", "quality_score": 80, "concept_type": "procedure", "output_keys": ["decomposition"], "improvement_goal": "Apply this procedure to run analysis consistently.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "apply", "levers": ["use a checklist", "document assumptions", "automate repeatable steps", "use guardrails for edge cases"], "warnings": ["Inconsistent procedures cause inconsistent results"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "time-series-components", "link_type": "related", "note": "Often learned alongside Time Series Components."}, {"to_slug": "stationarity", "link_type": "related", "note": "Often learned alongside Stationarity."}, {"to_slug": "acf", "link_type": "related", "note": "Often learned alongside Autocorrelation Function (ACF)."}], "formulas": [], "examples": [{"title": "Time Series Decomposition micro example", "scenario": "You are analyzing a dataset and need to apply: Time Series Decomposition.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "stationarity", "title": "Stationarity", "definition": "A stationary series has stable statistical properties over time (mean/variance/autocorrelation).", "plain_english": "Its behavior doesn’t drift unpredictably over time.", "when_to_use": "Use because many models (ARIMA) assume stationarity.", "tags": ["time-series"], "level": "intermediate", "status": "published", "quality_score": 80, "concept_type": "definition", "output_keys": ["stationarity"], "improvement_goal": "Use this concept to frame the analysis correctly.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "clarify", "levers": ["use consistent definitions", "align stakeholders on meaning", "document terms and units"], "warnings": ["Ambiguity causes misinterpretation"]}, "assumptions": "Use consistent definitions, units, and measurement processes.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "time-series-components", "link_type": "related", "note": "Often learned alongside Time Series Components."}, {"to_slug": "decomposition", "link_type": "related", "note": "Often learned alongside Time Series Decomposition."}, {"to_slug": "acf", "link_type": "related", "note": "Often learned alongside Autocorrelation Function (ACF)."}], "formulas": [], "examples": [{"title": "Stationarity micro example", "scenario": "You are analyzing a dataset and need to apply: Stationarity.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "acf", "title": "Autocorrelation Function (ACF)", "definition": "Correlation of a series with its lagged values across different lags.", "plain_english": "How today relates to yesterday/last week.", "when_to_use": "Use to identify persistence and choose ARIMA terms.", "tags": ["time-series"], "level": "intermediate", "status": "published", "quality_score": 80, "concept_type": "diagnostic", "output_keys": ["acf"], "improvement_goal": "Use this diagnostic to confirm model/process assumptions.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "diagnose", "levers": ["visualize residuals/errors", "segment diagnostics by subgroup", "address violations (transformations/robust methods)", "re-specify model"], "warnings": ["Ignoring diagnostics leads to biased conclusions"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.; Variables are measured appropriately.; For Pearson: relationship is roughly linear.", "limitations": "Correlation does not imply causation.; Outliers can strongly affect correlation.", "aliases": [], "links": [{"to_slug": "time-series-components", "link_type": "related", "note": "Often learned alongside Time Series Components."}, {"to_slug": "decomposition", "link_type": "related", "note": "Often learned alongside Time Series Decomposition."}, {"to_slug": "stationarity", "link_type": "related", "note": "Often learned alongside Stationarity."}], "formulas": [], "examples": [{"title": "Autocorrelation Function (ACF) micro example", "scenario": "You are analyzing a dataset and need to apply: Autocorrelation Function (ACF).", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "pacf", "title": "Partial Autocorrelation (PACF)", "definition": "Correlation between series and lag after removing effects of shorter lags.", "plain_english": "Direct lag relationship after accounting for earlier lags.", "when_to_use": "Use to select AR order in ARIMA.", "tags": ["time-series"], "level": "advanced", "status": "published", "quality_score": 80, "concept_type": "diagnostic", "output_keys": ["pacf"], "improvement_goal": "Use this diagnostic to confirm model/process assumptions.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "diagnose", "levers": ["visualize residuals/errors", "segment diagnostics by subgroup", "address violations (transformations/robust methods)", "re-specify model"], "warnings": ["Ignoring diagnostics leads to biased conclusions"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.; Variables are measured appropriately.; For Pearson: relationship is roughly linear.", "limitations": "Correlation does not imply causation.; Outliers can strongly affect correlation.", "aliases": [], "links": [{"to_slug": "time-series-components", "link_type": "related", "note": "Often learned alongside Time Series Components."}, {"to_slug": "decomposition", "link_type": "related", "note": "Often learned alongside Time Series Decomposition."}, {"to_slug": "stationarity", "link_type": "related", "note": "Often learned alongside Stationarity."}], "formulas": [], "examples": [{"title": "Partial Autocorrelation (PACF) micro example", "scenario": "You are analyzing a dataset and need to apply: Partial Autocorrelation (PACF).", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "moving-averages", "title": "Moving Averages", "definition": "Rolling average that smooths short-term fluctuations.", "plain_english": "A smoothed version of your trend.", "when_to_use": "Use for smoothing and baseline forecasting.", "tags": ["time-series"], "level": "intro", "status": "published", "quality_score": 80, "concept_type": "procedure", "output_keys": ["moving_average"], "improvement_goal": "Apply this procedure to run analysis consistently.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "apply", "levers": ["use a checklist", "document assumptions", "automate repeatable steps", "use guardrails for edge cases"], "warnings": ["Inconsistent procedures cause inconsistent results"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "time-series-components", "link_type": "related", "note": "Often learned alongside Time Series Components."}, {"to_slug": "decomposition", "link_type": "related", "note": "Often learned alongside Time Series Decomposition."}, {"to_slug": "stationarity", "link_type": "related", "note": "Often learned alongside Stationarity."}], "formulas": [], "examples": [{"title": "Moving Averages micro example", "scenario": "You are analyzing a dataset and need to apply: Moving Averages.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "exponential-smoothing", "title": "Exponential Smoothing", "definition": "Forecasting method weighting recent observations more heavily, with variants for trend/seasonality.", "plain_english": "Recent data matters more than old data.", "when_to_use": "Use for short-term forecasting with stable patterns.", "tags": ["time-series"], "level": "intermediate", "status": "published", "quality_score": 80, "concept_type": "procedure", "output_keys": ["exponential_smoothing"], "improvement_goal": "Apply this procedure to run analysis consistently.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "apply", "levers": ["use a checklist", "document assumptions", "automate repeatable steps", "use guardrails for edge cases"], "warnings": ["Inconsistent procedures cause inconsistent results"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "time-series-components", "link_type": "related", "note": "Often learned alongside Time Series Components."}, {"to_slug": "decomposition", "link_type": "related", "note": "Often learned alongside Time Series Decomposition."}, {"to_slug": "stationarity", "link_type": "related", "note": "Often learned alongside Stationarity."}], "formulas": [], "examples": [{"title": "Exponential Smoothing micro example", "scenario": "You are analyzing a dataset and need to apply: Exponential Smoothing.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "arima", "title": "ARIMA", "definition": "Autoregressive Integrated Moving Average model for forecasting stationary series (after differencing).", "plain_english": "A classic forecasting model using past values and errors.", "when_to_use": "Use for time series with autocorrelation and limited features.", "tags": ["time-series"], "level": "advanced", "status": "published", "quality_score": 80, "concept_type": "model", "output_keys": ["arima"], "improvement_goal": "Use this model to predict outcomes and understand drivers.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "improve", "levers": ["improve features", "use proper validation (CV/holdout)", "reduce leakage", "monitor drift and calibration"], "warnings": ["Good performance on training data can be misleading", "Predictive accuracy does not imply causality"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "time-series-components", "link_type": "related", "note": "Often learned alongside Time Series Components."}, {"to_slug": "decomposition", "link_type": "related", "note": "Often learned alongside Time Series Decomposition."}, {"to_slug": "stationarity", "link_type": "related", "note": "Often learned alongside Stationarity."}], "formulas": [], "examples": [{"title": "ARIMA micro example", "scenario": "You are analyzing a dataset and need to apply: ARIMA.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "sarima", "title": "SARIMA", "definition": "ARIMA extended with seasonal terms to model repeating seasonal patterns.", "plain_english": "ARIMA that handles seasonality.", "when_to_use": "Use when seasonality is present.", "tags": ["time-series"], "level": "advanced", "status": "published", "quality_score": 80, "concept_type": "model", "output_keys": ["sarima"], "improvement_goal": "Use this model to predict outcomes and understand drivers.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "improve", "levers": ["improve features", "use proper validation (CV/holdout)", "reduce leakage", "monitor drift and calibration"], "warnings": ["Good performance on training data can be misleading", "Predictive accuracy does not imply causality"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "time-series-components", "link_type": "related", "note": "Often learned alongside Time Series Components."}, {"to_slug": "decomposition", "link_type": "related", "note": "Often learned alongside Time Series Decomposition."}, {"to_slug": "stationarity", "link_type": "related", "note": "Often learned alongside Stationarity."}], "formulas": [], "examples": [{"title": "SARIMA micro example", "scenario": "You are analyzing a dataset and need to apply: SARIMA.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "prophet", "title": "Prophet (Conceptual)", "definition": "Forecasting approach combining trend + seasonality + holidays with robust fitting.", "plain_english": "A practical forecasting tool for business seasonality.", "when_to_use": "Use for business time series with multiple seasonalities and events.", "tags": ["time-series"], "level": "intermediate", "status": "published", "quality_score": 80, "concept_type": "model", "output_keys": ["prophet"], "improvement_goal": "Use this model to predict outcomes and understand drivers.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "improve", "levers": ["improve features", "use proper validation (CV/holdout)", "reduce leakage", "monitor drift and calibration"], "warnings": ["Good performance on training data can be misleading", "Predictive accuracy does not imply causality"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "time-series-components", "link_type": "related", "note": "Often learned alongside Time Series Components."}, {"to_slug": "decomposition", "link_type": "related", "note": "Often learned alongside Time Series Decomposition."}, {"to_slug": "stationarity", "link_type": "related", "note": "Often learned alongside Stationarity."}], "formulas": [], "examples": [{"title": "Prophet (Conceptual) micro example", "scenario": "You are analyzing a dataset and need to apply: Prophet (Conceptual).", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "mape", "title": "MAPE", "definition": "Mean Absolute Percentage Error: average absolute error as a percentage of actual values.", "plain_english": "Average percent error in forecasts.", "when_to_use": "Use to communicate forecast accuracy in intuitive terms (watch out for zeros).", "tags": ["forecasting", "metric"], "level": "intermediate", "status": "published", "quality_score": 80, "concept_type": "metric", "output_keys": ["mape"], "improvement_goal": "Measure and improve performance using this metric.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "optimize", "levers": ["define a target and guardrails", "segment by unit/time/category", "monitor trend and variation", "act on the biggest drivers"], "warnings": ["Do not optimize one metric while harming outcomes that matter more"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "time-series-components", "link_type": "related", "note": "Often learned alongside Time Series Components."}, {"to_slug": "decomposition", "link_type": "related", "note": "Often learned alongside Time Series Decomposition."}, {"to_slug": "stationarity", "link_type": "related", "note": "Often learned alongside Stationarity."}], "formulas": [], "examples": [{"title": "MAPE micro example", "scenario": "You are analyzing a dataset and need to apply: MAPE.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}], "description": "", "icon": "book", "sort_order": 0}, {"slug": "predictive-ml", "title": "Predictive Modeling & ML", "concepts": [{"slug": "train-test-split", "title": "Train/Test Split", "definition": "Partition data into training and testing sets to estimate generalization performance.", "plain_english": "Keep some data untouched to test honestly.", "when_to_use": "Use before modeling to avoid over-optimism.", "tags": ["validation"], "level": "intro", "status": "published", "quality_score": 80, "concept_type": "procedure", "output_keys": ["train_test_split"], "improvement_goal": "Build models you can trust in production.", "diagnostic_questions": ["Does performance hold on unseen data?", "Is there leakage or target contamination?", "Does the model stay stable over time and across subgroups?"], "improvement_playbook": {"direction": "apply", "levers": ["use a checklist", "document assumptions", "automate repeatable steps", "use guardrails for edge cases"], "warnings": ["Inconsistent procedures cause inconsistent results"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": ["train test split"], "links": [{"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}, {"to_slug": "bias-variance-tradeoff", "link_type": "related", "note": "Often learned alongside Bias–Variance Tradeoff."}], "formulas": [], "examples": [{"title": "Train/Test Split micro example", "scenario": "You are analyzing a dataset and need to apply: Train/Test Split.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "cross-validation", "title": "Cross-Validation", "definition": "Resampling method that trains and evaluates models across multiple folds of the data.", "plain_english": "Test the model multiple times on different slices.", "when_to_use": "Use for robust performance estimation and model selection.", "tags": ["validation"], "level": "intro", "status": "published", "quality_score": 80, "concept_type": "procedure", "output_keys": ["cross_validation", "cv"], "improvement_goal": "Build models you can trust in production.", "diagnostic_questions": ["Does performance hold on unseen data?", "Is there leakage or target contamination?", "Does the model stay stable over time and across subgroups?"], "improvement_playbook": {"direction": "apply", "levers": ["use a checklist", "document assumptions", "automate repeatable steps", "use guardrails for edge cases"], "warnings": ["Inconsistent procedures cause inconsistent results"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": ["cross validation"], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}, {"to_slug": "bias-variance-tradeoff", "link_type": "related", "note": "Often learned alongside Bias–Variance Tradeoff."}], "formulas": [], "examples": [{"title": "Cross-Validation micro example", "scenario": "You are analyzing a dataset and need to apply: Cross-Validation.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "stratified-cross-validation", "title": "Stratified Cross-Validation", "definition": "Cross-validation that preserves class proportions in each fold for classification problems.", "plain_english": "Keeps positives/negatives balanced in each split.", "when_to_use": "Use for imbalanced classification.", "tags": ["validation"], "level": "intermediate", "status": "published", "quality_score": 80, "concept_type": "procedure", "output_keys": ["stratified_cv"], "improvement_goal": "Build models you can trust in production.", "diagnostic_questions": ["Does performance hold on unseen data?", "Is there leakage or target contamination?", "Does the model stay stable over time and across subgroups?"], "improvement_playbook": {"direction": "apply", "levers": ["use a checklist", "document assumptions", "automate repeatable steps", "use guardrails for edge cases"], "warnings": ["Inconsistent procedures cause inconsistent results"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": ["stratified cross validation"], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "bias-variance-tradeoff", "link_type": "related", "note": "Often learned alongside Bias–Variance Tradeoff."}], "formulas": [], "examples": [{"title": "Stratified Cross-Validation micro example", "scenario": "You are analyzing a dataset and need to apply: Stratified Cross-Validation.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "bias-variance-tradeoff", "title": "Bias–Variance Tradeoff", "definition": "Tradeoff between underfitting (high bias) and overfitting (high variance) affecting generalization.", "plain_english": "Too simple misses patterns; too complex memorizes noise.", "when_to_use": "Use to choose model complexity and regularization.", "tags": ["modeling"], "level": "intermediate", "status": "published", "quality_score": 80, "concept_type": "definition", "output_keys": ["bias_variance"], "improvement_goal": "Use this concept to frame the analysis correctly.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "clarify", "levers": ["use consistent definitions", "align stakeholders on meaning", "document terms and units"], "warnings": ["Ambiguity causes misinterpretation"]}, "assumptions": "Use consistent definitions, units, and measurement processes.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": ["bias variance tradeoff", "var"], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}], "formulas": [], "examples": [{"title": "Bias–Variance Tradeoff micro example", "scenario": "You are analyzing a dataset and need to apply: Bias–Variance Tradeoff.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "feature-engineering", "title": "Feature Engineering", "definition": "Creating or transforming variables to improve model performance and interpretability.", "plain_english": "Design better inputs for the model.", "when_to_use": "Use to capture domain logic and reduce noise.", "tags": ["features"], "level": "intermediate", "status": "published", "quality_score": 80, "concept_type": "procedure", "output_keys": ["feature_engineering"], "improvement_goal": "Apply this procedure to run analysis consistently.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "apply", "levers": ["use a checklist", "document assumptions", "automate repeatable steps", "use guardrails for edge cases"], "warnings": ["Inconsistent procedures cause inconsistent results"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}], "formulas": [], "examples": [{"title": "Feature Engineering micro example", "scenario": "You are analyzing a dataset and need to apply: Feature Engineering.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "feature-selection", "title": "Feature Selection", "definition": "Selecting a subset of useful predictors to reduce overfitting and improve interpretability.", "plain_english": "Keep the drivers that matter most.", "when_to_use": "Use with many features or collinearity.", "tags": ["features"], "level": "intermediate", "status": "published", "quality_score": 80, "concept_type": "procedure", "output_keys": ["feature_selection"], "improvement_goal": "Apply this procedure to run analysis consistently.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "apply", "levers": ["use a checklist", "document assumptions", "automate repeatable steps", "use guardrails for edge cases"], "warnings": ["Inconsistent procedures cause inconsistent results"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}], "formulas": [], "examples": [{"title": "Feature Selection micro example", "scenario": "You are analyzing a dataset and need to apply: Feature Selection.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "hyperparameter-tuning", "title": "Hyperparameter Tuning", "definition": "Searching model settings (not learned from data) to improve performance under validation.", "plain_english": "Try different settings to find what works best.", "when_to_use": "Use with CV and a defined metric to avoid overfitting to the test set.", "tags": ["modeling"], "level": "intermediate", "status": "published", "quality_score": 80, "concept_type": "procedure", "output_keys": ["hyperparameter_tuning", "grid_search", "random_search"], "improvement_goal": "Apply this procedure to run analysis consistently.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "apply", "levers": ["use a checklist", "document assumptions", "automate repeatable steps", "use guardrails for edge cases"], "warnings": ["Inconsistent procedures cause inconsistent results"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}], "formulas": [], "examples": [{"title": "Hyperparameter Tuning micro example", "scenario": "You are analyzing a dataset and need to apply: Hyperparameter Tuning.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "learning-curves", "title": "Learning Curves", "definition": "Plots of performance vs training set size to diagnose data sufficiency and bias/variance issues.", "plain_english": "Shows whether more data will help.", "when_to_use": "Use to decide whether to collect more data or change the model.", "tags": ["modeling", "diagnostic"], "level": "advanced", "status": "published", "quality_score": 80, "concept_type": "chart", "output_keys": ["learning_curve"], "improvement_goal": "Use this visualization to spot patterns and variation.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "clarify", "levers": ["choose the right chart for the question", "label axes/units", "show distribution (not only averages)", "avoid misleading scales"], "warnings": ["Bad visuals create bad decisions"]}, "assumptions": "Use consistent definitions, units, and measurement processes.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}], "formulas": [], "examples": [{"title": "Learning Curves micro example", "scenario": "You are analyzing a dataset and need to apply: Learning Curves.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "decision-trees", "title": "Decision Trees", "definition": "Model that splits data by feature thresholds to predict outcomes.", "plain_english": "A flowchart of decisions.", "when_to_use": "Use for interpretable nonlinear relationships.", "tags": ["modeling"], "level": "intro", "status": "published", "quality_score": 80, "concept_type": "model", "output_keys": ["decision_tree"], "improvement_goal": "Use this model to predict outcomes and understand drivers.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "improve", "levers": ["improve features", "use proper validation (CV/holdout)", "reduce leakage", "monitor drift and calibration"], "warnings": ["Good performance on training data can be misleading", "Predictive accuracy does not imply causality"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}], "formulas": [], "examples": [{"title": "Decision Trees micro example", "scenario": "You are analyzing a dataset and need to apply: Decision Trees.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "random-forest", "title": "Random Forest", "definition": "Ensemble of decision trees trained on bootstrapped samples and random feature subsets.", "plain_english": "Many trees vote to improve accuracy and stability.", "when_to_use": "Use for strong baseline performance with tabular data.", "tags": ["modeling"], "level": "intermediate", "status": "published", "quality_score": 80, "concept_type": "model", "output_keys": ["random_forest"], "improvement_goal": "Use this model to predict outcomes and understand drivers.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "improve", "levers": ["improve features", "use proper validation (CV/holdout)", "reduce leakage", "monitor drift and calibration"], "warnings": ["Good performance on training data can be misleading", "Predictive accuracy does not imply causality"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}], "formulas": [], "examples": [{"title": "Random Forest micro example", "scenario": "You are analyzing a dataset and need to apply: Random Forest.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "gradient-boosting", "title": "Gradient Boosting", "definition": "Ensemble method that builds trees sequentially to correct previous errors.", "plain_english": "Trees that learn from mistakes.", "when_to_use": "Use for high-performance tabular prediction (with careful tuning).", "tags": ["modeling"], "level": "intermediate", "status": "published", "quality_score": 80, "concept_type": "model", "output_keys": ["gradient_boosting", "gbm"], "improvement_goal": "Use this model to predict outcomes and understand drivers.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "improve", "levers": ["improve features", "use proper validation (CV/holdout)", "reduce leakage", "monitor drift and calibration"], "warnings": ["Good performance on training data can be misleading", "Predictive accuracy does not imply causality"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}], "formulas": [], "examples": [{"title": "Gradient Boosting micro example", "scenario": "You are analyzing a dataset and need to apply: Gradient Boosting.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "xgboost", "title": "XGBoost (Conceptual)", "definition": "Optimized gradient boosting implementation with regularization and efficient training.", "plain_english": "A powerful boosted-tree engine.", "when_to_use": "Use when you need top performance on structured data.", "tags": ["modeling"], "level": "advanced", "status": "published", "quality_score": 80, "concept_type": "model", "output_keys": ["xgboost"], "improvement_goal": "Use this model to predict outcomes and understand drivers.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "improve", "levers": ["improve features", "use proper validation (CV/holdout)", "reduce leakage", "monitor drift and calibration"], "warnings": ["Good performance on training data can be misleading", "Predictive accuracy does not imply causality"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}], "formulas": [], "examples": [{"title": "XGBoost (Conceptual) micro example", "scenario": "You are analyzing a dataset and need to apply: XGBoost (Conceptual).", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "mae", "title": "MAE", "definition": "Mean Absolute Error: average absolute difference between predictions and actuals.", "plain_english": "On average, how far off you are (in units).", "when_to_use": "Use for robust error measurement less sensitive to outliers than MSE.", "tags": ["metrics"], "level": "intro", "status": "published", "quality_score": 80, "concept_type": "metric", "output_keys": ["mae"], "improvement_goal": "Measure and improve performance using this metric.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "optimize", "levers": ["define a target and guardrails", "segment by unit/time/category", "monitor trend and variation", "act on the biggest drivers"], "warnings": ["Do not optimize one metric while harming outcomes that matter more"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}], "formulas": [], "examples": [{"title": "MAE micro example", "scenario": "You are analyzing a dataset and need to apply: MAE.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "mse", "title": "MSE", "definition": "Mean Squared Error: average of squared prediction errors.", "plain_english": "Penalizes big mistakes more heavily.", "when_to_use": "Use when large errors are especially costly.", "tags": ["metrics"], "level": "intro", "status": "published", "quality_score": 80, "concept_type": "metric", "output_keys": ["mse"], "improvement_goal": "Measure and improve performance using this metric.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "optimize", "levers": ["define a target and guardrails", "segment by unit/time/category", "monitor trend and variation", "act on the biggest drivers"], "warnings": ["Do not optimize one metric while harming outcomes that matter more"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}], "formulas": [], "examples": [{"title": "MSE micro example", "scenario": "You are analyzing a dataset and need to apply: MSE.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "rmse", "title": "RMSE", "definition": "Root Mean Squared Error: square root of MSE, in original units.", "plain_english": "Typical size of your error (with emphasis on large misses).", "when_to_use": "Use when you want unit-based error comparable across models.", "tags": ["metrics"], "level": "intro", "status": "published", "quality_score": 80, "concept_type": "metric", "output_keys": ["rmse"], "improvement_goal": "Measure and improve performance using this metric.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "optimize", "levers": ["define a target and guardrails", "segment by unit/time/category", "monitor trend and variation", "act on the biggest drivers"], "warnings": ["Do not optimize one metric while harming outcomes that matter more"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}], "formulas": [], "examples": [{"title": "RMSE micro example", "scenario": "You are analyzing a dataset and need to apply: RMSE.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "log-loss", "title": "Log Loss", "definition": "Loss for probabilistic classification that penalizes confident wrong predictions heavily.", "plain_english": "Punishes being confidently wrong.", "when_to_use": "Use for evaluating predicted probabilities, not just class labels.", "tags": ["metrics"], "level": "advanced", "status": "published", "quality_score": 80, "concept_type": "metric", "output_keys": ["log_loss", "cross_entropy"], "improvement_goal": "Measure and improve performance using this metric.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "optimize", "levers": ["define a target and guardrails", "segment by unit/time/category", "monitor trend and variation", "act on the biggest drivers"], "warnings": ["Do not optimize one metric while harming outcomes that matter more"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}], "formulas": [], "examples": [{"title": "Log Loss micro example", "scenario": "You are analyzing a dataset and need to apply: Log Loss.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "brier-score", "title": "Brier Score", "definition": "Mean squared error of predicted probabilities for binary outcomes.", "plain_english": "How accurate and calibrated your probabilities are.", "when_to_use": "Use when probability quality matters (risk scores).", "tags": ["metrics"], "level": "advanced", "status": "published", "quality_score": 80, "concept_type": "metric", "output_keys": ["brier_score"], "improvement_goal": "Measure and improve performance using this metric.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "optimize", "levers": ["define a target and guardrails", "segment by unit/time/category", "monitor trend and variation", "act on the biggest drivers"], "warnings": ["Do not optimize one metric while harming outcomes that matter more"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}], "formulas": [], "examples": [{"title": "Brier Score micro example", "scenario": "You are analyzing a dataset and need to apply: Brier Score.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "confusion-matrix", "title": "Confusion Matrix", "definition": "Table of true/false positives/negatives summarizing classification outcomes.", "plain_english": "Shows what types of errors your classifier makes.", "when_to_use": "Use to understand tradeoffs and costs of mistakes.", "tags": ["metrics"], "level": "intro", "status": "published", "quality_score": 80, "concept_type": "metric", "output_keys": ["confusion_matrix"], "improvement_goal": "Measure and improve performance using this metric.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "optimize", "levers": ["define a target and guardrails", "segment by unit/time/category", "monitor trend and variation", "act on the biggest drivers"], "warnings": ["Do not optimize one metric while harming outcomes that matter more"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}], "formulas": [], "examples": [{"title": "Confusion Matrix micro example", "scenario": "You are analyzing a dataset and need to apply: Confusion Matrix.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "accuracy", "title": "Accuracy", "definition": "Proportion of correct predictions.", "plain_english": "How often the model is right.", "when_to_use": "Use when classes are balanced and error costs are similar.", "tags": ["metrics"], "level": "intro", "status": "published", "quality_score": 80, "concept_type": "metric", "output_keys": ["accuracy"], "improvement_goal": "Measure and improve performance using this metric.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "optimize", "levers": ["define a target and guardrails", "segment by unit/time/category", "monitor trend and variation", "act on the biggest drivers"], "warnings": ["Do not optimize one metric while harming outcomes that matter more"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}], "formulas": [], "examples": [{"title": "Accuracy micro example", "scenario": "You are analyzing a dataset and need to apply: Accuracy.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "precision", "title": "Precision", "definition": "Among predicted positives, the proportion that are truly positive.", "plain_english": "When the model says 'yes', how often is it correct?", "when_to_use": "Use when false positives are costly.", "tags": ["metrics"], "level": "intro", "status": "published", "quality_score": 80, "concept_type": "metric", "output_keys": ["precision"], "improvement_goal": "Measure and improve performance using this metric.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "optimize", "levers": ["define a target and guardrails", "segment by unit/time/category", "monitor trend and variation", "act on the biggest drivers"], "warnings": ["Do not optimize one metric while harming outcomes that matter more"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}], "formulas": [], "examples": [{"title": "Precision micro example", "scenario": "You are analyzing a dataset and need to apply: Precision.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "recall", "title": "Recall (Sensitivity)", "definition": "Among true positives, the proportion the model correctly identifies.", "plain_english": "How many real positives you catch.", "when_to_use": "Use when missing positives is costly.", "tags": ["metrics"], "level": "intro", "status": "published", "quality_score": 80, "concept_type": "metric", "output_keys": ["recall", "sensitivity"], "improvement_goal": "Measure and improve performance using this metric.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "optimize", "levers": ["define a target and guardrails", "segment by unit/time/category", "monitor trend and variation", "act on the biggest drivers"], "warnings": ["Do not optimize one metric while harming outcomes that matter more"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}], "formulas": [], "examples": [{"title": "Recall (Sensitivity) micro example", "scenario": "You are analyzing a dataset and need to apply: Recall (Sensitivity).", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "specificity", "title": "Specificity", "definition": "Among true negatives, the proportion correctly identified.", "plain_english": "How well you avoid false alarms.", "when_to_use": "Use with recall to understand tradeoffs.", "tags": ["metrics"], "level": "intermediate", "status": "published", "quality_score": 80, "concept_type": "metric", "output_keys": ["specificity"], "improvement_goal": "Measure and improve performance using this metric.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "optimize", "levers": ["define a target and guardrails", "segment by unit/time/category", "monitor trend and variation", "act on the biggest drivers"], "warnings": ["Do not optimize one metric while harming outcomes that matter more"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}], "formulas": [], "examples": [{"title": "Specificity micro example", "scenario": "You are analyzing a dataset and need to apply: Specificity.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}, {"slug": "f1-score", "title": "F1 Score", "definition": "Harmonic mean of precision and recall.", "plain_english": "A single score balancing misses and false alarms.", "when_to_use": "Use for imbalanced classes where both error types matter.", "tags": ["metrics"], "level": "intro", "status": "published", "quality_score": 80, "concept_type": "metric", "output_keys": ["f1"], "improvement_goal": "Measure and improve performance using this metric.", "diagnostic_questions": ["When does this apply to your question and data?", "What decision will you make based on the result?"], "improvement_playbook": {"direction": "optimize", "levers": ["define a target and guardrails", "segment by unit/time/category", "monitor trend and variation", "act on the biggest drivers"], "warnings": ["Do not optimize one metric while harming outcomes that matter more"]}, "assumptions": "Data are defined consistently and represent the population/process of interest.", "limitations": "Interpret results in context; poor data quality or bias can mislead conclusions.", "aliases": [], "links": [{"to_slug": "train-test-split", "link_type": "related", "note": "Often learned alongside Train/Test Split."}, {"to_slug": "cross-validation", "link_type": "related", "note": "Often learned alongside Cross-Validation."}, {"to_slug": "stratified-cross-validation", "link_type": "related", "note": "Often learned alongside Stratified Cross-Validation."}], "formulas": [], "examples": [{"title": "F1 Score micro example", "scenario": "You are analyzing a dataset and need to apply: F1 Score.", "solution": "Use the concept to compute/interpret the result, then decide a next step (clean data, check assumptions, or refine the model).", "dataset_json": null, "sort_order": 0}], "prerequisites": [{"prerequisite_slug": "what-is-data", "importance": "recommended"}], "resources": []}], "description": "", "icon": "book", "sort_order": 0}, {"slug": "model-reliability", "title": "Model Reliability & Validation", "concepts": [], "description": "", "icon": "book", "sort_order": 0}]}